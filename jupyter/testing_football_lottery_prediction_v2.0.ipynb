{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取全量的竞彩比赛列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host='localhost', user='root', password='breadt@2019', db='breadt-football-ml', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "with connection.cursor() as cursor:\n",
    "    sql = 'select * from `breadt_football_game_list`;'\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    train_game_list_df = pd.DataFrame(rows)\n",
    "    \n",
    "    connection.close()\n",
    "\n",
    "train_game_list_df['source'] = 'jc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取全量的胜负彩比赛列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host='localhost', user='root', password='breadt@2019', db='breadt-football-ml', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "with connection.cursor() as cursor:\n",
    "    sql = 'select * from `breadt_lottery_info`;'\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    train_lottery_game_list_df = pd.DataFrame(rows)\n",
    "    \n",
    "    connection.close()\n",
    "\n",
    "train_lottery_game_list_df['source'] = 'lottery'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并竞彩比赛列表和胜负彩比赛列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train_lottery_game_list_df.drop(['issue'], axis=1)\n",
    "df = pd.concat([train_game_list_df, tmp])\n",
    "df = df[['matchid', 'game', 'home_team', 'visit_team', 'gs', 'gd', 'gn', 'time', 'result', 'win_bet_return', 'draw_bet_return', 'lose_bet_return', 'source']]\n",
    "df = df.drop_duplicates(subset=['matchid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **设定训练范围** 并处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_group = ['澳超', '英超', '德甲', '德乙', '法甲', '西甲', '意甲', '日职', '英甲', '英冠', '苏超', '法乙', '葡超', '荷甲', '荷乙', '韩K联', '瑞典超', '挪超', '美职', '日乙', '俄超', '比甲', '瑞典甲', '法丙', '挪甲', '英乙', '苏冠', '巴甲', '智利甲', '墨超', '智利乙', '阿甲', '欧冠', '欧罗巴']\n",
    "match_group = ['澳超', '英超', '德甲', '德乙', '法甲', '西甲', '意甲', '日职', '英甲', '英冠', '苏超', '法乙', '葡超', '荷甲', '荷乙', '韩K联', '瑞典超', '挪超', '美职', '日乙', '俄超', '比甲', '瑞典甲', '法丙', '挪甲', '英乙', '苏冠', '巴甲', '智利甲', '墨超', '智利乙', '阿甲']\n",
    "match_df = df[(df['game'].isin(match_group))]\n",
    "match_df = match_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对team做encode 这个encoder后面预测的时候还会用到\n",
    "teams = list(set(df['home_team'].values) | set(df['visit_team'].values))\n",
    "team_encoder = preprocessing.LabelEncoder()\n",
    "team_encoder.fit(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_team(df):\n",
    "    df['home_team_encoder'] = team_encoder.transform(df['home_team'])\n",
    "    df['visit_team_encoder'] = team_encoder.transform(df['visit_team'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比赛名称encode\n",
    "games = list(set(match_df['game'].values))\n",
    "game_encoder = preprocessing.LabelEncoder()\n",
    "game_encoder.fit(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_game(df):\n",
    "    df['game_encoder'] = game_encoder.transform(df['game'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df['year'] = match_df.apply(lambda row: row.time.year, axis=1)\n",
    "match_df['month'] = match_df.apply(lambda row: row.time.month, axis=1)\n",
    "match_df['day'] = match_df.apply(lambda row: row.time.day, axis=1)\n",
    "match_df['fix_result'] = match_df.apply(lambda row: int(row.result) if row.result < 3 else 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = encode_team(match_df)\n",
    "match_df = encode_game(match_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 43477 entries, 1 to 12650\n",
      "Data columns (total 20 columns):\n",
      "matchid               43477 non-null int64\n",
      "game                  43477 non-null object\n",
      "home_team             43477 non-null object\n",
      "visit_team            43477 non-null object\n",
      "gs                    43477 non-null int64\n",
      "gd                    43477 non-null int64\n",
      "gn                    43477 non-null int64\n",
      "time                  43477 non-null datetime64[ns]\n",
      "result                43477 non-null int64\n",
      "win_bet_return        43477 non-null float64\n",
      "draw_bet_return       43477 non-null float64\n",
      "lose_bet_return       43477 non-null float64\n",
      "source                43477 non-null object\n",
      "year                  43477 non-null int64\n",
      "month                 43477 non-null int64\n",
      "day                   43477 non-null int64\n",
      "fix_result            43477 non-null int64\n",
      "home_team_encoder     43477 non-null int64\n",
      "visit_team_encoder    43477 non-null int64\n",
      "game_encoder          43477 non-null int64\n",
      "dtypes: datetime64[ns](1), float64(3), int64(12), object(4)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "match_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host='localhost', user='root', password='breadt@2019', db='breadt-football-ml', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "with connection.cursor() as cursor:\n",
    "    sql = 'select * from `breadt_football_recent_feature_info`;'\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    train_feature_df = pd.DataFrame(rows)\n",
    "    \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_goal_info(prefix, df):\n",
    "    target_cols = [\n",
    "        '_0_1_gd', \n",
    "        '_0_1_gs', \n",
    "        '_0_gd', \n",
    "        '_0_gs', \n",
    "        '_1_gd', \n",
    "        '_1_gs',\n",
    "        '_2_3_gd', \n",
    "        '_2_3_gs', \n",
    "        '_2_gd', \n",
    "        '_2_gs', \n",
    "        '_3_gd', \n",
    "        '_3_gs',\n",
    "        '_4_gd', \n",
    "        '_4_gs', \n",
    "        '_5_gd', \n",
    "        '_5_gs', \n",
    "        '_6_gd', \n",
    "        '_6_gs',\n",
    "        '_7_gd', \n",
    "        '_7_gs', \n",
    "        '_ab_4_gd', \n",
    "        '_ab_4_gs',\n",
    "        '_abs_draw', \n",
    "        '_abs_lose', \n",
    "        '_abs_win']\n",
    "\n",
    "    for k in target_cols:\n",
    "        df[prefix + k + '_rate'] = df[prefix + k] / df[prefix + '_count']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_goal_pref_info(prefix, df):\n",
    "    target_cols = [\n",
    "        '_draw', '_g', '_gd',\n",
    "        '_gs', '_lose', '_win',\n",
    "    ]\n",
    "\n",
    "    for k in target_cols:\n",
    "        df[prefix + k + '_rate'] = df[prefix + k] / df[prefix + '_count']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_df = take_goal_info('h', train_feature_df)\n",
    "train_feature_df = take_goal_info('v', train_feature_df)\n",
    "\n",
    "train_feature_df = take_goal_pref_info('h_host', train_feature_df)\n",
    "train_feature_df = take_goal_pref_info('v_visit', train_feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有球队的主客场进球平均数\n",
    "\n",
    "train_feature_df['h_avg_abs_gs'] = train_feature_df['h_abs_gs'].sum() / train_feature_df['h_count'].sum()\n",
    "train_feature_df['h_avg_abs_gd'] = train_feature_df['h_abs_gd'].sum() / train_feature_df['h_count'].sum()\n",
    "train_feature_df['v_avg_abs_gs'] = train_feature_df['v_abs_gs'].sum() / train_feature_df['v_count'].sum()\n",
    "train_feature_df['v_avg_abs_gd'] = train_feature_df['v_abs_gd'].sum() / train_feature_df['v_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_df['h_avg_abs_win'] = train_feature_df['h_abs_win'].sum() / train_feature_df['h_count'].sum()\n",
    "train_feature_df['h_avg_abs_draw'] = train_feature_df['h_abs_draw'].sum() / train_feature_df['h_count'].sum()\n",
    "train_feature_df['h_avg_abs_lose'] = train_feature_df['h_abs_lose'].sum() / train_feature_df['h_count'].sum()\n",
    "\n",
    "train_feature_df['v_avg_abs_win'] = train_feature_df['v_abs_win'].sum() / train_feature_df['v_count'].sum()\n",
    "train_feature_df['v_avg_abs_draw'] = train_feature_df['v_abs_draw'].sum() / train_feature_df['v_count'].sum()\n",
    "train_feature_df['v_avg_abs_lose'] = train_feature_df['v_abs_lose'].sum() / train_feature_df['v_count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取赔率信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host='localhost', user='root', password='breadt@2019', db='breadt-football-ml', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "with connection.cursor() as cursor:\n",
    "    sql = 'select * from `breadt_match_odd_info`;'\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    train_match_odd_df = pd.DataFrame(rows)\n",
    "    \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_init_draw_odd</th>\n",
       "      <th>avg_init_lose_odd</th>\n",
       "      <th>avg_init_win_odd</th>\n",
       "      <th>avg_new_draw_kelly</th>\n",
       "      <th>avg_new_draw_odd</th>\n",
       "      <th>avg_new_draw_rate</th>\n",
       "      <th>avg_new_lose_kelly</th>\n",
       "      <th>avg_new_lose_odd</th>\n",
       "      <th>avg_new_lose_rate</th>\n",
       "      <th>avg_new_win_kelly</th>\n",
       "      <th>...</th>\n",
       "      <th>min_new_lose_kelly</th>\n",
       "      <th>min_new_lose_odd</th>\n",
       "      <th>min_new_lose_rate</th>\n",
       "      <th>min_new_win_kelly</th>\n",
       "      <th>min_new_win_odd</th>\n",
       "      <th>min_new_win_rate</th>\n",
       "      <th>min_pay_rate</th>\n",
       "      <th>std_draw</th>\n",
       "      <th>std_lose</th>\n",
       "      <th>std_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.46</td>\n",
       "      <td>30.07</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>13.33</td>\n",
       "      <td>7.28</td>\n",
       "      <td>1.02</td>\n",
       "      <td>31.71</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.00</td>\n",
       "      <td>66.71</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1007.93</td>\n",
       "      <td>11029.60</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.38</td>\n",
       "      <td>4.29</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.43</td>\n",
       "      <td>26.55</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4.46</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.70</td>\n",
       "      <td>16.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.50</td>\n",
       "      <td>48.58</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.83</td>\n",
       "      <td>12.46</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.72</td>\n",
       "      <td>14.49</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.92</td>\n",
       "      <td>6.97</td>\n",
       "      <td>13.17</td>\n",
       "      <td>0.95</td>\n",
       "      <td>15.31</td>\n",
       "      <td>6.19</td>\n",
       "      <td>0.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.80</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.05</td>\n",
       "      <td>69.75</td>\n",
       "      <td>0.83</td>\n",
       "      <td>37.80</td>\n",
       "      <td>1120.81</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.02</td>\n",
       "      <td>17.87</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.92</td>\n",
       "      <td>7.89</td>\n",
       "      <td>11.68</td>\n",
       "      <td>0.94</td>\n",
       "      <td>17.85</td>\n",
       "      <td>5.29</td>\n",
       "      <td>0.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>10.50</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.02</td>\n",
       "      <td>75.49</td>\n",
       "      <td>0.83</td>\n",
       "      <td>93.17</td>\n",
       "      <td>1417.25</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.98</td>\n",
       "      <td>6.79</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4.06</td>\n",
       "      <td>22.43</td>\n",
       "      <td>0.92</td>\n",
       "      <td>7.17</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.69</td>\n",
       "      <td>5.35</td>\n",
       "      <td>9.54</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.30</td>\n",
       "      <td>59.57</td>\n",
       "      <td>0.83</td>\n",
       "      <td>4.23</td>\n",
       "      <td>90.73</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_init_draw_odd  avg_init_lose_odd  avg_init_win_odd  avg_new_draw_kelly  \\\n",
       "0              13.46              30.07              1.03                0.97   \n",
       "1               3.38               4.29              1.75                0.91   \n",
       "2               6.72              14.49              1.14                0.92   \n",
       "3               8.02              17.87              1.09                0.92   \n",
       "4               3.98               6.79              1.42                0.91   \n",
       "\n",
       "   avg_new_draw_odd  avg_new_draw_rate  avg_new_lose_kelly  avg_new_lose_odd  \\\n",
       "0             13.33               7.28                1.02             31.71   \n",
       "1              3.43              26.55                0.91              4.46   \n",
       "2              6.97              13.17                0.95             15.31   \n",
       "3              7.89              11.68                0.94             17.85   \n",
       "4              4.06              22.43                0.92              7.17   \n",
       "\n",
       "   avg_new_lose_rate  avg_new_win_kelly  ...  min_new_lose_kelly  \\\n",
       "0               3.21               0.92  ...                0.18   \n",
       "1              20.50               0.91  ...                0.76   \n",
       "2               6.19               0.91  ...                0.48   \n",
       "3               5.29               0.91  ...                0.56   \n",
       "4              12.88               0.91  ...                0.69   \n",
       "\n",
       "   min_new_lose_odd  min_new_lose_rate  min_new_win_kelly  min_new_win_odd  \\\n",
       "0              5.50               1.17               0.90             1.00   \n",
       "1              3.70              16.79               0.79             1.50   \n",
       "2              7.80               3.38               0.85             1.05   \n",
       "3             10.50               2.86               0.85             1.02   \n",
       "4              5.35               9.54               0.84             1.30   \n",
       "\n",
       "   min_new_win_rate  min_pay_rate  std_draw  std_lose  std_win  \n",
       "0             66.71          0.83   1007.93  11029.60     0.04  \n",
       "1             48.58          0.83      1.83     12.46     0.35  \n",
       "2             69.75          0.83     37.80   1120.81     0.05  \n",
       "3             75.49          0.83     93.17   1417.25     0.04  \n",
       "4             59.57          0.83      4.23     90.73     0.14  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_match_odd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30980 entries, 0 to 43278\n",
      "Columns: 217 entries, matchid to std_win\n",
      "dtypes: datetime64[ns](1), float64(200), int64(12), object(4)\n",
      "memory usage: 51.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_dataset_df = pd.merge(match_df, train_feature_df, on='matchid', how='left')\n",
    "train_dataset_df = pd.merge(train_dataset_df, train_match_odd_df, on='matchid', how='left')\n",
    "train_dataset_df = train_dataset_df.dropna()\n",
    "train_dataset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['matchid', 'game', 'home_team', 'visit_team', 'gs', 'gd', 'gn',\n",
       "       'time', 'result', 'win_bet_return', 'draw_bet_return',\n",
       "       'lose_bet_return', 'source', 'year', 'month', 'day', 'fix_result',\n",
       "       'home_team_encoder', 'visit_team_encoder', 'game_encoder',\n",
       "       'h_0_1_gd', 'h_0_1_gs', 'h_0_gd', 'h_0_gs', 'h_1_gd', 'h_1_gs',\n",
       "       'h_2_3_gd', 'h_2_3_gs', 'h_2_gd', 'h_2_gs', 'h_3_gd', 'h_3_gs',\n",
       "       'h_4_gd', 'h_4_gs', 'h_5_gd', 'h_5_gs', 'h_6_gd', 'h_6_gs',\n",
       "       'h_7_gd', 'h_7_gs', 'h_ab_4_gd', 'h_ab_4_gs', 'h_abs_avg_g',\n",
       "       'h_abs_avg_gd', 'h_abs_avg_gs', 'h_abs_draw', 'h_abs_g',\n",
       "       'h_abs_gd', 'h_abs_gs', 'h_abs_lose', 'h_abs_win', 'h_count',\n",
       "       'h_host_count', 'h_host_draw', 'h_host_g', 'h_host_gd',\n",
       "       'h_host_gs', 'h_host_lose', 'h_host_win', 'id_x', 'v_0_1_gd',\n",
       "       'v_0_1_gs', 'v_0_gd', 'v_0_gs', 'v_1_gd', 'v_1_gs', 'v_2_3_gd',\n",
       "       'v_2_3_gs', 'v_2_gd', 'v_2_gs', 'v_3_gd', 'v_3_gs', 'v_4_gd',\n",
       "       'v_4_gs', 'v_5_gd', 'v_5_gs', 'v_6_gd', 'v_6_gs', 'v_7_gd',\n",
       "       'v_7_gs', 'v_ab_4_gd', 'v_ab_4_gs', 'v_abs_avg_g', 'v_abs_avg_gd',\n",
       "       'v_abs_avg_gs', 'v_abs_draw', 'v_abs_g', 'v_abs_gd', 'v_abs_gs',\n",
       "       'v_abs_lose', 'v_abs_win', 'v_count', 'v_visit_count',\n",
       "       'v_visit_draw', 'v_visit_g', 'v_visit_gd', 'v_visit_gs',\n",
       "       'v_visit_lose', 'v_visit_win', 'h_0_1_gd_rate', 'h_0_1_gs_rate',\n",
       "       'h_0_gd_rate', 'h_0_gs_rate', 'h_1_gd_rate', 'h_1_gs_rate',\n",
       "       'h_2_3_gd_rate', 'h_2_3_gs_rate', 'h_2_gd_rate', 'h_2_gs_rate',\n",
       "       'h_3_gd_rate', 'h_3_gs_rate', 'h_4_gd_rate', 'h_4_gs_rate',\n",
       "       'h_5_gd_rate', 'h_5_gs_rate', 'h_6_gd_rate', 'h_6_gs_rate',\n",
       "       'h_7_gd_rate', 'h_7_gs_rate', 'h_ab_4_gd_rate', 'h_ab_4_gs_rate',\n",
       "       'h_abs_draw_rate', 'h_abs_lose_rate', 'h_abs_win_rate',\n",
       "       'v_0_1_gd_rate', 'v_0_1_gs_rate', 'v_0_gd_rate', 'v_0_gs_rate',\n",
       "       'v_1_gd_rate', 'v_1_gs_rate', 'v_2_3_gd_rate', 'v_2_3_gs_rate',\n",
       "       'v_2_gd_rate', 'v_2_gs_rate', 'v_3_gd_rate', 'v_3_gs_rate',\n",
       "       'v_4_gd_rate', 'v_4_gs_rate', 'v_5_gd_rate', 'v_5_gs_rate',\n",
       "       'v_6_gd_rate', 'v_6_gs_rate', 'v_7_gd_rate', 'v_7_gs_rate',\n",
       "       'v_ab_4_gd_rate', 'v_ab_4_gs_rate', 'v_abs_draw_rate',\n",
       "       'v_abs_lose_rate', 'v_abs_win_rate', 'h_host_draw_rate',\n",
       "       'h_host_g_rate', 'h_host_gd_rate', 'h_host_gs_rate',\n",
       "       'h_host_lose_rate', 'h_host_win_rate', 'v_visit_draw_rate',\n",
       "       'v_visit_g_rate', 'v_visit_gd_rate', 'v_visit_gs_rate',\n",
       "       'v_visit_lose_rate', 'v_visit_win_rate', 'h_avg_abs_gs',\n",
       "       'h_avg_abs_gd', 'v_avg_abs_gs', 'v_avg_abs_gd', 'h_avg_abs_win',\n",
       "       'h_avg_abs_draw', 'h_avg_abs_lose', 'v_avg_abs_win',\n",
       "       'v_avg_abs_draw', 'v_avg_abs_lose', 'avg_init_draw_odd',\n",
       "       'avg_init_lose_odd', 'avg_init_win_odd', 'avg_new_draw_kelly',\n",
       "       'avg_new_draw_odd', 'avg_new_draw_rate', 'avg_new_lose_kelly',\n",
       "       'avg_new_lose_odd', 'avg_new_lose_rate', 'avg_new_win_kelly',\n",
       "       'avg_new_win_odd', 'avg_new_win_rate', 'avg_pay_rate',\n",
       "       'dispersion_draw', 'dispersion_lose', 'dispersion_win', 'id_y',\n",
       "       'max_init_draw_odd', 'max_init_lose_odd', 'max_init_win_odd',\n",
       "       'max_new_draw_kelly', 'max_new_draw_odd', 'max_new_draw_rate',\n",
       "       'max_new_lose_kelly', 'max_new_lose_odd', 'max_new_lose_rate',\n",
       "       'max_new_win_kelly', 'max_new_win_odd', 'max_new_win_rate',\n",
       "       'max_pay_rate', 'min_init_draw_odd', 'min_init_lose_odd',\n",
       "       'min_init_win_odd', 'min_new_draw_kelly', 'min_new_draw_odd',\n",
       "       'min_new_draw_rate', 'min_new_lose_kelly', 'min_new_lose_odd',\n",
       "       'min_new_lose_rate', 'min_new_win_kelly', 'min_new_win_odd',\n",
       "       'min_new_win_rate', 'min_pay_rate', 'std_draw', 'std_lose',\n",
       "       'std_win'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb regressor 尝试预测分差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_df['fix_result'] = train_dataset_df['gs'] - train_dataset_df['gd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.72901\tval-rmse:1.76074\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[1]\ttrain-rmse:1.72535\tval-rmse:1.75802\n",
      "[2]\ttrain-rmse:1.72187\tval-rmse:1.75476\n",
      "[3]\ttrain-rmse:1.71838\tval-rmse:1.7519\n",
      "[4]\ttrain-rmse:1.71494\tval-rmse:1.74894\n",
      "[5]\ttrain-rmse:1.71153\tval-rmse:1.74608\n",
      "[6]\ttrain-rmse:1.70818\tval-rmse:1.74288\n",
      "[7]\ttrain-rmse:1.70491\tval-rmse:1.74039\n",
      "[8]\ttrain-rmse:1.7016\tval-rmse:1.73806\n",
      "[9]\ttrain-rmse:1.69843\tval-rmse:1.73527\n",
      "[10]\ttrain-rmse:1.69529\tval-rmse:1.73271\n",
      "[11]\ttrain-rmse:1.69229\tval-rmse:1.73002\n",
      "[12]\ttrain-rmse:1.68925\tval-rmse:1.72717\n",
      "[13]\ttrain-rmse:1.68621\tval-rmse:1.72499\n",
      "[14]\ttrain-rmse:1.68326\tval-rmse:1.72203\n",
      "[15]\ttrain-rmse:1.68033\tval-rmse:1.71957\n",
      "[16]\ttrain-rmse:1.67751\tval-rmse:1.71712\n",
      "[17]\ttrain-rmse:1.67468\tval-rmse:1.71478\n",
      "[18]\ttrain-rmse:1.67186\tval-rmse:1.71214\n",
      "[19]\ttrain-rmse:1.66906\tval-rmse:1.70982\n",
      "[20]\ttrain-rmse:1.66635\tval-rmse:1.70753\n",
      "[21]\ttrain-rmse:1.66366\tval-rmse:1.70514\n",
      "[22]\ttrain-rmse:1.66096\tval-rmse:1.70279\n",
      "[23]\ttrain-rmse:1.6583\tval-rmse:1.70086\n",
      "[24]\ttrain-rmse:1.65569\tval-rmse:1.69871\n",
      "[25]\ttrain-rmse:1.65316\tval-rmse:1.69671\n",
      "[26]\ttrain-rmse:1.65062\tval-rmse:1.69481\n",
      "[27]\ttrain-rmse:1.64823\tval-rmse:1.69268\n",
      "[28]\ttrain-rmse:1.6458\tval-rmse:1.69067\n",
      "[29]\ttrain-rmse:1.64329\tval-rmse:1.68897\n",
      "[30]\ttrain-rmse:1.64085\tval-rmse:1.68724\n",
      "[31]\ttrain-rmse:1.6385\tval-rmse:1.6862\n",
      "[32]\ttrain-rmse:1.63612\tval-rmse:1.68472\n",
      "[33]\ttrain-rmse:1.63391\tval-rmse:1.68303\n",
      "[34]\ttrain-rmse:1.63176\tval-rmse:1.68099\n",
      "[35]\ttrain-rmse:1.62956\tval-rmse:1.67901\n",
      "[36]\ttrain-rmse:1.62729\tval-rmse:1.67692\n",
      "[37]\ttrain-rmse:1.62516\tval-rmse:1.67532\n",
      "[38]\ttrain-rmse:1.62306\tval-rmse:1.67363\n",
      "[39]\ttrain-rmse:1.62101\tval-rmse:1.67219\n",
      "[40]\ttrain-rmse:1.61891\tval-rmse:1.67061\n",
      "[41]\ttrain-rmse:1.61686\tval-rmse:1.66902\n",
      "[42]\ttrain-rmse:1.61485\tval-rmse:1.66751\n",
      "[43]\ttrain-rmse:1.61292\tval-rmse:1.66651\n",
      "[44]\ttrain-rmse:1.61102\tval-rmse:1.66519\n",
      "[45]\ttrain-rmse:1.60915\tval-rmse:1.66371\n",
      "[46]\ttrain-rmse:1.60725\tval-rmse:1.66234\n",
      "[47]\ttrain-rmse:1.60539\tval-rmse:1.66144\n",
      "[48]\ttrain-rmse:1.6035\tval-rmse:1.65981\n",
      "[49]\ttrain-rmse:1.60172\tval-rmse:1.65833\n",
      "[50]\ttrain-rmse:1.5999\tval-rmse:1.65708\n",
      "[51]\ttrain-rmse:1.59815\tval-rmse:1.65582\n",
      "[52]\ttrain-rmse:1.59631\tval-rmse:1.65456\n",
      "[53]\ttrain-rmse:1.59451\tval-rmse:1.65369\n",
      "[54]\ttrain-rmse:1.59282\tval-rmse:1.65298\n",
      "[55]\ttrain-rmse:1.59119\tval-rmse:1.65215\n",
      "[56]\ttrain-rmse:1.58952\tval-rmse:1.65081\n",
      "[57]\ttrain-rmse:1.58792\tval-rmse:1.64963\n",
      "[58]\ttrain-rmse:1.58627\tval-rmse:1.64858\n",
      "[59]\ttrain-rmse:1.58466\tval-rmse:1.64752\n",
      "[60]\ttrain-rmse:1.5831\tval-rmse:1.64651\n",
      "[61]\ttrain-rmse:1.5816\tval-rmse:1.64521\n",
      "[62]\ttrain-rmse:1.58009\tval-rmse:1.64447\n",
      "[63]\ttrain-rmse:1.57852\tval-rmse:1.64358\n",
      "[64]\ttrain-rmse:1.57699\tval-rmse:1.6425\n",
      "[65]\ttrain-rmse:1.57538\tval-rmse:1.64124\n",
      "[66]\ttrain-rmse:1.57389\tval-rmse:1.64047\n",
      "[67]\ttrain-rmse:1.57253\tval-rmse:1.63956\n",
      "[68]\ttrain-rmse:1.57108\tval-rmse:1.63842\n",
      "[69]\ttrain-rmse:1.56971\tval-rmse:1.63734\n",
      "[70]\ttrain-rmse:1.56828\tval-rmse:1.63678\n",
      "[71]\ttrain-rmse:1.56686\tval-rmse:1.63585\n",
      "[72]\ttrain-rmse:1.56551\tval-rmse:1.63506\n",
      "[73]\ttrain-rmse:1.5642\tval-rmse:1.63426\n",
      "[74]\ttrain-rmse:1.56291\tval-rmse:1.63372\n",
      "[75]\ttrain-rmse:1.56153\tval-rmse:1.63291\n",
      "[76]\ttrain-rmse:1.56025\tval-rmse:1.63204\n",
      "[77]\ttrain-rmse:1.5589\tval-rmse:1.63142\n",
      "[78]\ttrain-rmse:1.55771\tval-rmse:1.63083\n",
      "[79]\ttrain-rmse:1.55653\tval-rmse:1.63022\n",
      "[80]\ttrain-rmse:1.55529\tval-rmse:1.62931\n",
      "[81]\ttrain-rmse:1.55403\tval-rmse:1.62865\n",
      "[82]\ttrain-rmse:1.55284\tval-rmse:1.62785\n",
      "[83]\ttrain-rmse:1.55159\tval-rmse:1.62697\n",
      "[84]\ttrain-rmse:1.55043\tval-rmse:1.626\n",
      "[85]\ttrain-rmse:1.54931\tval-rmse:1.62533\n",
      "[86]\ttrain-rmse:1.54827\tval-rmse:1.62464\n",
      "[87]\ttrain-rmse:1.54709\tval-rmse:1.62437\n",
      "[88]\ttrain-rmse:1.54591\tval-rmse:1.62371\n",
      "[89]\ttrain-rmse:1.5449\tval-rmse:1.62331\n",
      "[90]\ttrain-rmse:1.54382\tval-rmse:1.62252\n",
      "[91]\ttrain-rmse:1.54275\tval-rmse:1.62194\n",
      "[92]\ttrain-rmse:1.54174\tval-rmse:1.6214\n",
      "[93]\ttrain-rmse:1.54063\tval-rmse:1.62075\n",
      "[94]\ttrain-rmse:1.53952\tval-rmse:1.62007\n",
      "[95]\ttrain-rmse:1.53844\tval-rmse:1.61949\n",
      "[96]\ttrain-rmse:1.53736\tval-rmse:1.61919\n",
      "[97]\ttrain-rmse:1.53635\tval-rmse:1.61852\n",
      "[98]\ttrain-rmse:1.53538\tval-rmse:1.61806\n",
      "[99]\ttrain-rmse:1.53435\tval-rmse:1.6176\n",
      "[100]\ttrain-rmse:1.53331\tval-rmse:1.61724\n",
      "[101]\ttrain-rmse:1.53226\tval-rmse:1.61682\n",
      "[102]\ttrain-rmse:1.53124\tval-rmse:1.61641\n",
      "[103]\ttrain-rmse:1.53034\tval-rmse:1.61611\n",
      "[104]\ttrain-rmse:1.52933\tval-rmse:1.61555\n",
      "[105]\ttrain-rmse:1.52832\tval-rmse:1.61511\n",
      "[106]\ttrain-rmse:1.52746\tval-rmse:1.61459\n",
      "[107]\ttrain-rmse:1.52641\tval-rmse:1.61423\n",
      "[108]\ttrain-rmse:1.52548\tval-rmse:1.61426\n",
      "[109]\ttrain-rmse:1.5246\tval-rmse:1.61401\n",
      "[110]\ttrain-rmse:1.52364\tval-rmse:1.61346\n",
      "[111]\ttrain-rmse:1.5228\tval-rmse:1.61301\n",
      "[112]\ttrain-rmse:1.52187\tval-rmse:1.61262\n",
      "[113]\ttrain-rmse:1.52099\tval-rmse:1.61214\n",
      "[114]\ttrain-rmse:1.52021\tval-rmse:1.61174\n",
      "[115]\ttrain-rmse:1.51932\tval-rmse:1.6114\n",
      "[116]\ttrain-rmse:1.51841\tval-rmse:1.61148\n",
      "[117]\ttrain-rmse:1.51751\tval-rmse:1.61134\n",
      "[118]\ttrain-rmse:1.51659\tval-rmse:1.61123\n",
      "[119]\ttrain-rmse:1.51576\tval-rmse:1.61119\n",
      "[120]\ttrain-rmse:1.51496\tval-rmse:1.61093\n",
      "[121]\ttrain-rmse:1.51407\tval-rmse:1.61061\n",
      "[122]\ttrain-rmse:1.5132\tval-rmse:1.61057\n",
      "[123]\ttrain-rmse:1.51231\tval-rmse:1.6103\n",
      "[124]\ttrain-rmse:1.51166\tval-rmse:1.60992\n",
      "[125]\ttrain-rmse:1.51097\tval-rmse:1.60987\n",
      "[126]\ttrain-rmse:1.51018\tval-rmse:1.60977\n",
      "[127]\ttrain-rmse:1.50951\tval-rmse:1.60948\n",
      "[128]\ttrain-rmse:1.50867\tval-rmse:1.60954\n",
      "[129]\ttrain-rmse:1.50784\tval-rmse:1.60921\n",
      "[130]\ttrain-rmse:1.50711\tval-rmse:1.60907\n",
      "[131]\ttrain-rmse:1.50639\tval-rmse:1.60897\n",
      "[132]\ttrain-rmse:1.5057\tval-rmse:1.60896\n",
      "[133]\ttrain-rmse:1.50495\tval-rmse:1.60866\n",
      "[134]\ttrain-rmse:1.50414\tval-rmse:1.60874\n",
      "[135]\ttrain-rmse:1.50342\tval-rmse:1.60839\n",
      "[136]\ttrain-rmse:1.5028\tval-rmse:1.60821\n",
      "[137]\ttrain-rmse:1.50196\tval-rmse:1.60782\n",
      "[138]\ttrain-rmse:1.5013\tval-rmse:1.60736\n",
      "[139]\ttrain-rmse:1.50063\tval-rmse:1.60728\n",
      "[140]\ttrain-rmse:1.49994\tval-rmse:1.60696\n",
      "[141]\ttrain-rmse:1.49917\tval-rmse:1.60695\n",
      "[142]\ttrain-rmse:1.49851\tval-rmse:1.60656\n",
      "[143]\ttrain-rmse:1.49787\tval-rmse:1.60621\n",
      "[144]\ttrain-rmse:1.49711\tval-rmse:1.60596\n",
      "[145]\ttrain-rmse:1.49643\tval-rmse:1.60576\n",
      "[146]\ttrain-rmse:1.49584\tval-rmse:1.60536\n",
      "[147]\ttrain-rmse:1.49522\tval-rmse:1.60514\n",
      "[148]\ttrain-rmse:1.49456\tval-rmse:1.60485\n",
      "[149]\ttrain-rmse:1.49403\tval-rmse:1.60462\n",
      "[150]\ttrain-rmse:1.49329\tval-rmse:1.60451\n",
      "[151]\ttrain-rmse:1.49264\tval-rmse:1.60448\n",
      "[152]\ttrain-rmse:1.49195\tval-rmse:1.60409\n",
      "[153]\ttrain-rmse:1.49131\tval-rmse:1.60404\n",
      "[154]\ttrain-rmse:1.49071\tval-rmse:1.60387\n",
      "[155]\ttrain-rmse:1.49013\tval-rmse:1.60372\n",
      "[156]\ttrain-rmse:1.48939\tval-rmse:1.60356\n",
      "[157]\ttrain-rmse:1.48889\tval-rmse:1.60319\n",
      "[158]\ttrain-rmse:1.48827\tval-rmse:1.6029\n",
      "[159]\ttrain-rmse:1.48772\tval-rmse:1.60262\n",
      "[160]\ttrain-rmse:1.48711\tval-rmse:1.60247\n",
      "[161]\ttrain-rmse:1.48645\tval-rmse:1.60213\n",
      "[162]\ttrain-rmse:1.48593\tval-rmse:1.60222\n",
      "[163]\ttrain-rmse:1.48528\tval-rmse:1.60207\n",
      "[164]\ttrain-rmse:1.48469\tval-rmse:1.6019\n",
      "[165]\ttrain-rmse:1.4841\tval-rmse:1.60175\n",
      "[166]\ttrain-rmse:1.48348\tval-rmse:1.60149\n",
      "[167]\ttrain-rmse:1.48289\tval-rmse:1.60115\n",
      "[168]\ttrain-rmse:1.48231\tval-rmse:1.60107\n",
      "[169]\ttrain-rmse:1.48173\tval-rmse:1.60136\n",
      "[170]\ttrain-rmse:1.48114\tval-rmse:1.60127\n",
      "[171]\ttrain-rmse:1.48051\tval-rmse:1.60105\n",
      "[172]\ttrain-rmse:1.47987\tval-rmse:1.60114\n",
      "[173]\ttrain-rmse:1.47928\tval-rmse:1.60111\n",
      "[174]\ttrain-rmse:1.47862\tval-rmse:1.60088\n",
      "[175]\ttrain-rmse:1.47813\tval-rmse:1.601\n",
      "[176]\ttrain-rmse:1.47758\tval-rmse:1.60078\n",
      "[177]\ttrain-rmse:1.47705\tval-rmse:1.60044\n",
      "[178]\ttrain-rmse:1.47654\tval-rmse:1.60019\n",
      "[179]\ttrain-rmse:1.47596\tval-rmse:1.59992\n",
      "[180]\ttrain-rmse:1.47546\tval-rmse:1.59974\n",
      "[181]\ttrain-rmse:1.47487\tval-rmse:1.59961\n",
      "[182]\ttrain-rmse:1.47429\tval-rmse:1.59948\n",
      "[183]\ttrain-rmse:1.47379\tval-rmse:1.5996\n",
      "[184]\ttrain-rmse:1.4733\tval-rmse:1.59926\n",
      "[185]\ttrain-rmse:1.47279\tval-rmse:1.59921\n",
      "[186]\ttrain-rmse:1.47211\tval-rmse:1.59927\n",
      "[187]\ttrain-rmse:1.47168\tval-rmse:1.599\n",
      "[188]\ttrain-rmse:1.4713\tval-rmse:1.59896\n",
      "[189]\ttrain-rmse:1.47085\tval-rmse:1.59897\n",
      "[190]\ttrain-rmse:1.47043\tval-rmse:1.59891\n",
      "[191]\ttrain-rmse:1.46993\tval-rmse:1.59872\n",
      "[192]\ttrain-rmse:1.46943\tval-rmse:1.59867\n",
      "[193]\ttrain-rmse:1.46897\tval-rmse:1.59856\n",
      "[194]\ttrain-rmse:1.46843\tval-rmse:1.59859\n",
      "[195]\ttrain-rmse:1.46801\tval-rmse:1.59825\n",
      "[196]\ttrain-rmse:1.46756\tval-rmse:1.59816\n",
      "[197]\ttrain-rmse:1.46699\tval-rmse:1.59801\n",
      "[198]\ttrain-rmse:1.46644\tval-rmse:1.59781\n",
      "[199]\ttrain-rmse:1.46596\tval-rmse:1.59769\n",
      "[200]\ttrain-rmse:1.46545\tval-rmse:1.5975\n",
      "[201]\ttrain-rmse:1.46503\tval-rmse:1.5976\n",
      "[202]\ttrain-rmse:1.46455\tval-rmse:1.59748\n",
      "[203]\ttrain-rmse:1.46393\tval-rmse:1.59749\n",
      "[204]\ttrain-rmse:1.46332\tval-rmse:1.59735\n",
      "[205]\ttrain-rmse:1.46278\tval-rmse:1.59729\n",
      "[206]\ttrain-rmse:1.46231\tval-rmse:1.59728\n",
      "[207]\ttrain-rmse:1.46183\tval-rmse:1.59726\n",
      "[208]\ttrain-rmse:1.46145\tval-rmse:1.59721\n",
      "[209]\ttrain-rmse:1.46099\tval-rmse:1.59736\n",
      "[210]\ttrain-rmse:1.46053\tval-rmse:1.59737\n",
      "[211]\ttrain-rmse:1.46007\tval-rmse:1.59735\n",
      "[212]\ttrain-rmse:1.4597\tval-rmse:1.5973\n",
      "[213]\ttrain-rmse:1.45917\tval-rmse:1.59729\n",
      "[214]\ttrain-rmse:1.4586\tval-rmse:1.59716\n",
      "[215]\ttrain-rmse:1.45807\tval-rmse:1.5971\n",
      "[216]\ttrain-rmse:1.45776\tval-rmse:1.59707\n",
      "[217]\ttrain-rmse:1.45735\tval-rmse:1.59716\n",
      "[218]\ttrain-rmse:1.45698\tval-rmse:1.59704\n",
      "[219]\ttrain-rmse:1.45646\tval-rmse:1.59705\n",
      "[220]\ttrain-rmse:1.45599\tval-rmse:1.59705\n",
      "[221]\ttrain-rmse:1.45569\tval-rmse:1.59697\n",
      "[222]\ttrain-rmse:1.45519\tval-rmse:1.59667\n",
      "[223]\ttrain-rmse:1.45472\tval-rmse:1.59676\n",
      "[224]\ttrain-rmse:1.45439\tval-rmse:1.59677\n",
      "[225]\ttrain-rmse:1.4538\tval-rmse:1.59708\n",
      "[226]\ttrain-rmse:1.45334\tval-rmse:1.59708\n",
      "[227]\ttrain-rmse:1.45301\tval-rmse:1.59718\n",
      "[228]\ttrain-rmse:1.4525\tval-rmse:1.59709\n",
      "[229]\ttrain-rmse:1.45209\tval-rmse:1.59707\n",
      "[230]\ttrain-rmse:1.45173\tval-rmse:1.59703\n",
      "[231]\ttrain-rmse:1.4512\tval-rmse:1.59708\n",
      "[232]\ttrain-rmse:1.45069\tval-rmse:1.59712\n",
      "[233]\ttrain-rmse:1.4502\tval-rmse:1.59718\n",
      "[234]\ttrain-rmse:1.44979\tval-rmse:1.59712\n",
      "[235]\ttrain-rmse:1.44947\tval-rmse:1.5972\n",
      "[236]\ttrain-rmse:1.44921\tval-rmse:1.5972\n",
      "[237]\ttrain-rmse:1.44879\tval-rmse:1.5968\n",
      "[238]\ttrain-rmse:1.44846\tval-rmse:1.5968\n",
      "[239]\ttrain-rmse:1.44799\tval-rmse:1.59685\n",
      "[240]\ttrain-rmse:1.44765\tval-rmse:1.59681\n",
      "[241]\ttrain-rmse:1.44717\tval-rmse:1.59665\n",
      "[242]\ttrain-rmse:1.44683\tval-rmse:1.59693\n",
      "[243]\ttrain-rmse:1.44627\tval-rmse:1.59685\n",
      "[244]\ttrain-rmse:1.44586\tval-rmse:1.59671\n",
      "[245]\ttrain-rmse:1.44541\tval-rmse:1.59657\n",
      "[246]\ttrain-rmse:1.44508\tval-rmse:1.59648\n",
      "[247]\ttrain-rmse:1.44481\tval-rmse:1.59645\n",
      "[248]\ttrain-rmse:1.44444\tval-rmse:1.59643\n",
      "[249]\ttrain-rmse:1.44392\tval-rmse:1.59654\n",
      "[250]\ttrain-rmse:1.44352\tval-rmse:1.59653\n",
      "[251]\ttrain-rmse:1.44321\tval-rmse:1.59656\n",
      "[252]\ttrain-rmse:1.44283\tval-rmse:1.59676\n",
      "[253]\ttrain-rmse:1.44246\tval-rmse:1.59675\n",
      "[254]\ttrain-rmse:1.44219\tval-rmse:1.59685\n",
      "[255]\ttrain-rmse:1.44182\tval-rmse:1.59699\n",
      "[256]\ttrain-rmse:1.4413\tval-rmse:1.59694\n",
      "[257]\ttrain-rmse:1.44097\tval-rmse:1.59701\n",
      "[258]\ttrain-rmse:1.44059\tval-rmse:1.59692\n",
      "[259]\ttrain-rmse:1.44022\tval-rmse:1.59682\n",
      "[260]\ttrain-rmse:1.43991\tval-rmse:1.59699\n",
      "[261]\ttrain-rmse:1.43959\tval-rmse:1.59692\n",
      "[262]\ttrain-rmse:1.43928\tval-rmse:1.5964\n",
      "[263]\ttrain-rmse:1.43896\tval-rmse:1.59638\n",
      "[264]\ttrain-rmse:1.4386\tval-rmse:1.59648\n",
      "[265]\ttrain-rmse:1.43811\tval-rmse:1.59636\n",
      "[266]\ttrain-rmse:1.43764\tval-rmse:1.59661\n",
      "[267]\ttrain-rmse:1.43723\tval-rmse:1.59654\n",
      "[268]\ttrain-rmse:1.43693\tval-rmse:1.59666\n",
      "[269]\ttrain-rmse:1.43648\tval-rmse:1.59654\n",
      "[270]\ttrain-rmse:1.43604\tval-rmse:1.5965\n",
      "[271]\ttrain-rmse:1.43558\tval-rmse:1.59651\n",
      "[272]\ttrain-rmse:1.43524\tval-rmse:1.59656\n",
      "[273]\ttrain-rmse:1.43467\tval-rmse:1.59662\n",
      "[274]\ttrain-rmse:1.43433\tval-rmse:1.59654\n",
      "[275]\ttrain-rmse:1.4338\tval-rmse:1.59648\n",
      "[276]\ttrain-rmse:1.43337\tval-rmse:1.59637\n",
      "[277]\ttrain-rmse:1.43308\tval-rmse:1.59633\n",
      "[278]\ttrain-rmse:1.4327\tval-rmse:1.59633\n",
      "[279]\ttrain-rmse:1.43232\tval-rmse:1.59631\n",
      "[280]\ttrain-rmse:1.43198\tval-rmse:1.59612\n",
      "[281]\ttrain-rmse:1.43164\tval-rmse:1.59598\n",
      "[282]\ttrain-rmse:1.43129\tval-rmse:1.59594\n",
      "[283]\ttrain-rmse:1.43086\tval-rmse:1.59601\n",
      "[284]\ttrain-rmse:1.43039\tval-rmse:1.59605\n",
      "[285]\ttrain-rmse:1.43015\tval-rmse:1.59608\n",
      "[286]\ttrain-rmse:1.42984\tval-rmse:1.59598\n",
      "[287]\ttrain-rmse:1.42962\tval-rmse:1.59595\n",
      "[288]\ttrain-rmse:1.42936\tval-rmse:1.59605\n",
      "[289]\ttrain-rmse:1.42895\tval-rmse:1.59607\n",
      "[290]\ttrain-rmse:1.4285\tval-rmse:1.59603\n",
      "[291]\ttrain-rmse:1.4282\tval-rmse:1.59605\n",
      "[292]\ttrain-rmse:1.42787\tval-rmse:1.59606\n",
      "[293]\ttrain-rmse:1.42762\tval-rmse:1.59604\n",
      "[294]\ttrain-rmse:1.42709\tval-rmse:1.59617\n",
      "[295]\ttrain-rmse:1.42666\tval-rmse:1.59617\n",
      "[296]\ttrain-rmse:1.4263\tval-rmse:1.59636\n",
      "[297]\ttrain-rmse:1.42587\tval-rmse:1.59597\n",
      "[298]\ttrain-rmse:1.42548\tval-rmse:1.59605\n",
      "[299]\ttrain-rmse:1.42504\tval-rmse:1.59612\n",
      "[300]\ttrain-rmse:1.42464\tval-rmse:1.5963\n",
      "[301]\ttrain-rmse:1.42433\tval-rmse:1.59618\n",
      "[302]\ttrain-rmse:1.42399\tval-rmse:1.59609\n",
      "[303]\ttrain-rmse:1.42367\tval-rmse:1.59612\n",
      "[304]\ttrain-rmse:1.42322\tval-rmse:1.59612\n",
      "[305]\ttrain-rmse:1.4229\tval-rmse:1.59617\n",
      "[306]\ttrain-rmse:1.42268\tval-rmse:1.59624\n",
      "[307]\ttrain-rmse:1.4223\tval-rmse:1.59664\n",
      "[308]\ttrain-rmse:1.42188\tval-rmse:1.59661\n",
      "[309]\ttrain-rmse:1.42154\tval-rmse:1.59661\n",
      "[310]\ttrain-rmse:1.42128\tval-rmse:1.59655\n",
      "[311]\ttrain-rmse:1.42097\tval-rmse:1.59654\n",
      "[312]\ttrain-rmse:1.42054\tval-rmse:1.59646\n",
      "[313]\ttrain-rmse:1.42019\tval-rmse:1.59633\n",
      "[314]\ttrain-rmse:1.41988\tval-rmse:1.5961\n",
      "[315]\ttrain-rmse:1.41957\tval-rmse:1.59612\n",
      "[316]\ttrain-rmse:1.41933\tval-rmse:1.59597\n",
      "[317]\ttrain-rmse:1.41881\tval-rmse:1.59612\n",
      "[318]\ttrain-rmse:1.41866\tval-rmse:1.59618\n",
      "[319]\ttrain-rmse:1.41841\tval-rmse:1.59618\n",
      "[320]\ttrain-rmse:1.41795\tval-rmse:1.59661\n",
      "[321]\ttrain-rmse:1.41752\tval-rmse:1.59664\n",
      "[322]\ttrain-rmse:1.41705\tval-rmse:1.59665\n",
      "[323]\ttrain-rmse:1.41682\tval-rmse:1.59669\n",
      "[324]\ttrain-rmse:1.41642\tval-rmse:1.59669\n",
      "[325]\ttrain-rmse:1.41608\tval-rmse:1.59682\n",
      "[326]\ttrain-rmse:1.41577\tval-rmse:1.59675\n",
      "[327]\ttrain-rmse:1.41557\tval-rmse:1.59673\n",
      "[328]\ttrain-rmse:1.41508\tval-rmse:1.59682\n",
      "[329]\ttrain-rmse:1.4148\tval-rmse:1.59673\n",
      "[330]\ttrain-rmse:1.41452\tval-rmse:1.59673\n",
      "[331]\ttrain-rmse:1.41415\tval-rmse:1.59669\n",
      "[332]\ttrain-rmse:1.41382\tval-rmse:1.59684\n",
      "[333]\ttrain-rmse:1.41349\tval-rmse:1.59675\n",
      "[334]\ttrain-rmse:1.41332\tval-rmse:1.59685\n",
      "[335]\ttrain-rmse:1.41303\tval-rmse:1.5968\n",
      "[336]\ttrain-rmse:1.41249\tval-rmse:1.59694\n",
      "[337]\ttrain-rmse:1.41215\tval-rmse:1.59687\n",
      "[338]\ttrain-rmse:1.41171\tval-rmse:1.597\n",
      "[339]\ttrain-rmse:1.41153\tval-rmse:1.59692\n",
      "[340]\ttrain-rmse:1.41129\tval-rmse:1.59689\n",
      "[341]\ttrain-rmse:1.41108\tval-rmse:1.59704\n",
      "[342]\ttrain-rmse:1.41069\tval-rmse:1.59709\n",
      "[343]\ttrain-rmse:1.41034\tval-rmse:1.59711\n",
      "[344]\ttrain-rmse:1.40998\tval-rmse:1.59704\n",
      "[345]\ttrain-rmse:1.40961\tval-rmse:1.59684\n",
      "[346]\ttrain-rmse:1.4093\tval-rmse:1.59679\n",
      "[347]\ttrain-rmse:1.40902\tval-rmse:1.59682\n",
      "[348]\ttrain-rmse:1.40851\tval-rmse:1.59677\n",
      "[349]\ttrain-rmse:1.40829\tval-rmse:1.59686\n",
      "[350]\ttrain-rmse:1.40801\tval-rmse:1.59686\n",
      "[351]\ttrain-rmse:1.40775\tval-rmse:1.59679\n",
      "[352]\ttrain-rmse:1.40734\tval-rmse:1.59693\n",
      "[353]\ttrain-rmse:1.40698\tval-rmse:1.59721\n",
      "[354]\ttrain-rmse:1.40657\tval-rmse:1.59724\n",
      "[355]\ttrain-rmse:1.40629\tval-rmse:1.59733\n",
      "[356]\ttrain-rmse:1.40603\tval-rmse:1.5972\n",
      "[357]\ttrain-rmse:1.40564\tval-rmse:1.59718\n",
      "[358]\ttrain-rmse:1.40536\tval-rmse:1.5973\n",
      "[359]\ttrain-rmse:1.40502\tval-rmse:1.59744\n",
      "[360]\ttrain-rmse:1.40482\tval-rmse:1.59754\n",
      "[361]\ttrain-rmse:1.40443\tval-rmse:1.59765\n",
      "[362]\ttrain-rmse:1.40404\tval-rmse:1.59773\n",
      "[363]\ttrain-rmse:1.40364\tval-rmse:1.5976\n",
      "[364]\ttrain-rmse:1.40336\tval-rmse:1.59748\n",
      "[365]\ttrain-rmse:1.40312\tval-rmse:1.59743\n",
      "[366]\ttrain-rmse:1.40275\tval-rmse:1.59724\n",
      "[367]\ttrain-rmse:1.40236\tval-rmse:1.59724\n",
      "[368]\ttrain-rmse:1.40196\tval-rmse:1.59711\n",
      "[369]\ttrain-rmse:1.40162\tval-rmse:1.59713\n",
      "[370]\ttrain-rmse:1.40127\tval-rmse:1.5972\n",
      "[371]\ttrain-rmse:1.40091\tval-rmse:1.59709\n",
      "[372]\ttrain-rmse:1.40046\tval-rmse:1.59708\n",
      "[373]\ttrain-rmse:1.40014\tval-rmse:1.59699\n",
      "[374]\ttrain-rmse:1.39974\tval-rmse:1.59682\n",
      "[375]\ttrain-rmse:1.39936\tval-rmse:1.59685\n",
      "[376]\ttrain-rmse:1.39892\tval-rmse:1.59691\n",
      "[377]\ttrain-rmse:1.3987\tval-rmse:1.59684\n",
      "[378]\ttrain-rmse:1.39821\tval-rmse:1.59681\n",
      "[379]\ttrain-rmse:1.39784\tval-rmse:1.59671\n",
      "[380]\ttrain-rmse:1.39762\tval-rmse:1.5968\n",
      "[381]\ttrain-rmse:1.39722\tval-rmse:1.5967\n",
      "[382]\ttrain-rmse:1.39703\tval-rmse:1.5967\n",
      "Stopping. Best iteration:\n",
      "[282]\ttrain-rmse:1.43129\tval-rmse:1.59594\n",
      "\n",
      "282\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "x_columns = [\n",
    "#     'win_bet_return', 'draw_bet_return', 'lose_bet_return', \n",
    "#     'year', 'month',\n",
    "#     'home_team_encoder', 'visit_team_encoder', 'game_encoder',\n",
    "    \n",
    "    'h_avg_abs_gs',\n",
    "    'h_avg_abs_gd', 'v_avg_abs_gs', 'v_avg_abs_gd', 'h_avg_abs_win',\n",
    "    'h_avg_abs_draw', 'h_avg_abs_lose', 'v_avg_abs_win',\n",
    "    'v_avg_abs_draw', 'v_avg_abs_lose',\n",
    "    \n",
    "    'h_0_1_gd', \n",
    "    'h_0_1_gs', \n",
    "    'h_0_gd', \n",
    "    'h_0_gs', \n",
    "    'h_1_gd', \n",
    "    'h_1_gs',\n",
    "    'h_2_3_gd', \n",
    "    'h_2_3_gs', \n",
    "    'h_2_gd', 'h_2_gs', 'h_3_gd', 'h_3_gs',\n",
    "    'h_4_gd', 'h_4_gs', 'h_5_gd', 'h_5_gs', 'h_6_gd', 'h_6_gs',\n",
    "    'h_7_gd', 'h_7_gs', 'h_ab_4_gd', 'h_ab_4_gs', 'h_abs_avg_g',\n",
    "    'h_abs_avg_gd', 'h_abs_avg_gs', 'h_abs_draw', 'h_abs_g',\n",
    "    'h_abs_gd', 'h_abs_gs', 'h_abs_lose', 'h_abs_win', 'h_count',\n",
    "    'h_host_count', 'h_host_draw', 'h_host_g', 'h_host_gd',\n",
    "    'h_host_gs', 'h_host_lose', 'h_host_win', \n",
    "    \n",
    "    'v_0_1_gd',\n",
    "    'v_0_1_gs', 'v_0_gd', 'v_0_gs', 'v_1_gd', 'v_1_gs', 'v_2_3_gd',\n",
    "    'v_2_3_gs', 'v_2_gd', 'v_2_gs', 'v_3_gd', 'v_3_gs', 'v_4_gd',\n",
    "    'v_4_gs', 'v_5_gd', 'v_5_gs', 'v_6_gd', 'v_6_gs', 'v_7_gd',\n",
    "    'v_7_gs', 'v_ab_4_gd', 'v_ab_4_gs', 'v_abs_avg_g', 'v_abs_avg_gd',\n",
    "    'v_abs_avg_gs', 'v_abs_draw', 'v_abs_g', 'v_abs_gd', 'v_abs_gs',\n",
    "    'v_abs_lose', 'v_abs_win', \n",
    "    \n",
    "    'v_count', \n",
    "    'v_visit_count',\n",
    "    'v_visit_draw', 'v_visit_g', 'v_visit_gd', 'v_visit_gs',\n",
    "    'v_visit_lose', 'v_visit_win',\n",
    "    \n",
    "    'h_0_1_gd_rate', 'h_0_1_gs_rate',\n",
    "    'h_0_gd_rate', 'h_0_gs_rate', 'h_1_gd_rate', 'h_1_gs_rate',\n",
    "    'h_2_3_gd_rate', 'h_2_3_gs_rate', 'h_2_gd_rate', 'h_2_gs_rate',\n",
    "    'h_3_gd_rate', 'h_3_gs_rate', 'h_4_gd_rate', 'h_4_gs_rate',\n",
    "    'h_5_gd_rate', 'h_5_gs_rate', 'h_6_gd_rate', 'h_6_gs_rate',\n",
    "    'h_7_gd_rate', 'h_7_gs_rate', 'h_ab_4_gd_rate', 'h_ab_4_gs_rate',\n",
    "    'h_abs_draw_rate', 'h_abs_lose_rate', 'h_abs_win_rate',\n",
    "    'v_0_1_gd_rate', 'v_0_1_gs_rate', 'v_0_gd_rate', 'v_0_gs_rate',\n",
    "    'v_1_gd_rate', 'v_1_gs_rate', 'v_2_3_gd_rate', 'v_2_3_gs_rate',\n",
    "    'v_2_gd_rate', 'v_2_gs_rate', 'v_3_gd_rate', 'v_3_gs_rate',\n",
    "    'v_4_gd_rate', 'v_4_gs_rate', 'v_5_gd_rate', 'v_5_gs_rate',\n",
    "    'v_6_gd_rate', 'v_6_gs_rate', 'v_7_gd_rate', 'v_7_gs_rate',\n",
    "    \n",
    "    'v_ab_4_gd_rate', \n",
    "    'v_ab_4_gs_rate', \n",
    "    'v_abs_draw_rate',\n",
    "    'v_abs_lose_rate', \n",
    "    'v_abs_win_rate',\n",
    "    'h_host_draw_rate',\n",
    "    'h_host_g_rate', \n",
    "    'h_host_gd_rate', \n",
    "    'h_host_gs_rate',\n",
    "    'h_host_lose_rate', \n",
    "    'h_host_win_rate', \n",
    "    'v_visit_draw_rate',\n",
    "    'v_visit_g_rate', \n",
    "    'v_visit_gd_rate', \n",
    "    'v_visit_gs_rate',\n",
    "    'v_visit_lose_rate', \n",
    "    'v_visit_win_rate',\n",
    "    \n",
    "    'avg_init_draw_odd',\n",
    "    'avg_init_lose_odd', 'avg_init_win_odd', 'avg_new_draw_kelly',\n",
    "    'avg_new_draw_odd', 'avg_new_draw_rate', 'avg_new_lose_kelly',\n",
    "    'avg_new_lose_odd', 'avg_new_lose_rate', 'avg_new_win_kelly',\n",
    "    'avg_new_win_odd', 'avg_new_win_rate', 'avg_pay_rate',\n",
    "    'dispersion_draw', 'dispersion_lose', 'dispersion_win', 'id_y',\n",
    "    'max_init_draw_odd', 'max_init_lose_odd', 'max_init_win_odd',\n",
    "    'max_new_draw_kelly', 'max_new_draw_odd', 'max_new_draw_rate',\n",
    "    'max_new_lose_kelly', 'max_new_lose_odd', 'max_new_lose_rate',\n",
    "    'max_new_win_kelly', 'max_new_win_odd', 'max_new_win_rate',\n",
    "    'max_pay_rate', 'min_init_draw_odd', 'min_init_lose_odd',\n",
    "    'min_init_win_odd', 'min_new_draw_kelly', 'min_new_draw_odd',\n",
    "    'min_new_draw_rate', 'min_new_lose_kelly', 'min_new_lose_odd',\n",
    "    'min_new_lose_rate', 'min_new_win_kelly', 'min_new_win_odd',\n",
    "    'min_new_win_rate', 'min_pay_rate', 'std_draw', 'std_lose',\n",
    "    'std_win'\n",
    "]\n",
    "    \n",
    "params={\n",
    "    'booster':'gbtree',\n",
    "    # 这里手写数字是0-9，是一个多类的问题，因此采用了multisoft多分类器，\n",
    "    'objective': 'reg:linear', \n",
    "#     'objective': 'multi:softprob',\n",
    "#     'num_class':3, # 类数，与 multisoftmax 并用\n",
    "    \n",
    "    'gamma':0.01,  # 在树的叶子节点下一个分区的最小损失，越大算法模型越保守 。[0:]\n",
    "    'max_depth':8, # 构建树的深度 [1:]\n",
    "    \n",
    "    #'lambda':450,  # L2 正则项权重\n",
    "    'subsample':0.7, # 采样训练数据，设置为0.5，随机选择一般的数据实例 (0:1]\n",
    "    'colsample_bytree':0.7, # 构建树树时的采样比率 (0:1]\n",
    "    #'min_child_weight':12, # 节点的最少特征数\n",
    "    'silent':1 ,\n",
    "    \n",
    "#     这部分需要调整\n",
    "#     'eta': 0.05, # 如同学习率\n",
    "    'eta': 0.01, # 如同学习率\n",
    "    \n",
    "    \n",
    "    'seed':2018,\n",
    "    'nthread':4,# cpu 线程数,根据自己U的个数适当调整\n",
    "}\n",
    "\n",
    "t = train_dataset_df\n",
    "\n",
    "train_dataset = t[t['year'] < 2019]\n",
    "test_dataset = t[t['year'] == 2019]\n",
    "\n",
    "valid_dataset = test_dataset[test_dataset['month'] < 3]\n",
    "test_dataset = test_dataset[test_dataset['month'] >= 3]\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_dataset[x_columns], label=train_dataset['fix_result'])\n",
    "xgtest = xgb.DMatrix(test_dataset[x_columns], label=test_dataset['fix_result'])\n",
    "xgvalid = xgb.DMatrix(valid_dataset[x_columns], label=valid_dataset['fix_result'])\n",
    "\n",
    "watchlist = [(xgtrain, 'train'),(xgvalid, 'val')]\n",
    "\n",
    "num_rounds = 10000\n",
    "stop_rounds = 100\n",
    "\n",
    "# num_rounds = 10000\n",
    "# stop_rounds = 300\n",
    "\n",
    "model = xgb.train(params, xgtrain, num_rounds, watchlist, early_stopping_rounds=stop_rounds)\n",
    "print(model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14979899,  0.10005543,  1.000135  , ...,  1.1058667 ,\n",
       "        0.8398075 , -0.6734235 ], dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gs</th>\n",
       "      <th>gd</th>\n",
       "      <th>fix_result</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40535</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>-0.149799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41590</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.100055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41851</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41852</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.224394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41853</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.444732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41855</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41856</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.133074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41857</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.026531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41858</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.088672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41859</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.614954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41861</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-0.118094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41862</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>-0.110142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41863</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.678307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41864</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.320429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41866</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.159383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41869</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.719266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41876</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.367239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41877</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.330523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41878</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.460778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gs  gd  fix_result     preds\n",
       "40535   0   3          -3 -0.149799\n",
       "41590   1   2          -1  0.100055\n",
       "41851   1   1           0  1.000135\n",
       "41852   2   1           1  0.224394\n",
       "41853   2   0           2  0.444732\n",
       "41855   0   0           0  0.397154\n",
       "41856   0   1          -1  1.133074\n",
       "41857   0   2          -2  0.026531\n",
       "41858   1   0           1 -0.088672\n",
       "41859   0   2          -2  0.614954\n",
       "41860   0   0           0  0.383085\n",
       "41861   0   4          -4 -0.118094\n",
       "41862   0   3          -3 -0.110142\n",
       "41863   2   0           2  0.678307\n",
       "41864   2   2           0 -0.320429\n",
       "41866   1   1           0  0.159383\n",
       "41869   2   0           2  0.719266\n",
       "41876   4   0           4  0.367239\n",
       "41877   0   2          -2  0.330523\n",
       "41878   2   0           2  0.460778"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['preds'] = preds\n",
    "test_dataset[['gs', 'gd', 'fix_result', 'preds']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(-0.14979899), math.ceil(0.14979899)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_score(row):\n",
    "    if row.preds > 0:\n",
    "        return -(math.floor(row.preds)) + 0.5\n",
    "    else:\n",
    "        return -(math.ceil(row.preds)) - 0.5\n",
    "\n",
    "# test_dataset['rq'] = test_dataset.apply(lambda row: math.floor(math.floor(row.preds * 10) / 5) * 0.5, axis=1)\n",
    "test_dataset['rq'] = test_dataset.apply(lambda row: get_score(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6074270557029178\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchid</th>\n",
       "      <th>gs</th>\n",
       "      <th>gd</th>\n",
       "      <th>fix_result</th>\n",
       "      <th>rq</th>\n",
       "      <th>rq_result</th>\n",
       "      <th>pred_rq_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40535</th>\n",
       "      <td>2432035</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41590</th>\n",
       "      <td>2432303</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41851</th>\n",
       "      <td>2514240</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41852</th>\n",
       "      <td>2514242</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41853</th>\n",
       "      <td>2437256</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41855</th>\n",
       "      <td>2514244</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41856</th>\n",
       "      <td>2411805</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41857</th>\n",
       "      <td>2415112</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41858</th>\n",
       "      <td>2415109</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41859</th>\n",
       "      <td>2406862</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41860</th>\n",
       "      <td>2406864</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41861</th>\n",
       "      <td>2406863</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41862</th>\n",
       "      <td>2406865</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41863</th>\n",
       "      <td>2406868</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41864</th>\n",
       "      <td>2406866</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41866</th>\n",
       "      <td>2405433</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41869</th>\n",
       "      <td>2406364</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41876</th>\n",
       "      <td>2411058</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41877</th>\n",
       "      <td>2436047</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41878</th>\n",
       "      <td>2428851</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       matchid  gs  gd  fix_result   rq  rq_result  pred_rq_result\n",
       "40535  2432035   0   3          -3 -0.5       -3.5               0\n",
       "41590  2432303   1   2          -1  0.5       -0.5               0\n",
       "41851  2514240   1   1           0 -0.5       -0.5               0\n",
       "41852  2514242   2   1           1  0.5        1.5               1\n",
       "41853  2437256   2   0           2  0.5        2.5               1\n",
       "41855  2514244   0   0           0  0.5        0.5               1\n",
       "41856  2411805   0   1          -1 -0.5       -1.5               0\n",
       "41857  2415112   0   2          -2  0.5       -1.5               0\n",
       "41858  2415109   1   0           1 -0.5        0.5               1\n",
       "41859  2406862   0   2          -2  0.5       -1.5               0\n",
       "41860  2406864   0   0           0  0.5        0.5               1\n",
       "41861  2406863   0   4          -4 -0.5       -4.5               0\n",
       "41862  2406865   0   3          -3 -0.5       -3.5               0\n",
       "41863  2406868   2   0           2  0.5        2.5               1\n",
       "41864  2406866   2   2           0 -0.5       -0.5               0\n",
       "41866  2405433   1   1           0  0.5        0.5               1\n",
       "41869  2406364   2   0           2  0.5        2.5               1\n",
       "41876  2411058   4   0           4  0.5        4.5               1\n",
       "41877  2436047   0   2          -2  0.5       -1.5               0\n",
       "41878  2428851   2   0           2  0.5        2.5               1"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['rq_result'] = test_dataset['rq'] + test_dataset['fix_result']\n",
    "test_dataset['pred_rq_result'] = test_dataset.apply(lambda row: 1 if row.rq_result > 0 else 0, axis=1)\n",
    "\n",
    "# a = test_dataset[test_dataset['rq'] >= 0]\n",
    "a = test_dataset\n",
    "print(len(a[a['pred_rq_result'] == 1])/ len(a))\n",
    "a[['matchid', 'gs', 'gd', 'fix_result', 'rq', 'rq_result', 'pred_rq_result']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  0.5,  1.5, -1.5, -3.5])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['rq'].drop_duplicates().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30980\n",
      "[0]\ttrain-merror:0.46384\tval-merror:0.536404\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 100 rounds.\n",
      "[1]\ttrain-merror:0.45606\tval-merror:0.536404\n",
      "[2]\ttrain-merror:0.453146\tval-merror:0.534918\n",
      "[3]\ttrain-merror:0.453695\tval-merror:0.533432\n",
      "[4]\ttrain-merror:0.453935\tval-merror:0.523031\n",
      "[5]\ttrain-merror:0.455066\tval-merror:0.526003\n",
      "[6]\ttrain-merror:0.454209\tval-merror:0.534918\n",
      "[7]\ttrain-merror:0.452667\tval-merror:0.531946\n",
      "[8]\ttrain-merror:0.450747\tval-merror:0.523031\n",
      "[9]\ttrain-merror:0.45061\tval-merror:0.524517\n",
      "[10]\ttrain-merror:0.450644\tval-merror:0.524517\n",
      "[11]\ttrain-merror:0.450781\tval-merror:0.527489\n",
      "[12]\ttrain-merror:0.450713\tval-merror:0.530461\n",
      "[13]\ttrain-merror:0.451056\tval-merror:0.530461\n",
      "[14]\ttrain-merror:0.450233\tval-merror:0.527489\n",
      "[15]\ttrain-merror:0.449925\tval-merror:0.524517\n",
      "[16]\ttrain-merror:0.45037\tval-merror:0.526003\n",
      "[17]\ttrain-merror:0.449685\tval-merror:0.527489\n",
      "[18]\ttrain-merror:0.450473\tval-merror:0.528975\n",
      "[19]\ttrain-merror:0.450404\tval-merror:0.527489\n",
      "[20]\ttrain-merror:0.450644\tval-merror:0.524517\n",
      "[21]\ttrain-merror:0.450233\tval-merror:0.528975\n",
      "[22]\ttrain-merror:0.44989\tval-merror:0.527489\n",
      "[23]\ttrain-merror:0.449445\tval-merror:0.524517\n",
      "[24]\ttrain-merror:0.448725\tval-merror:0.520059\n",
      "[25]\ttrain-merror:0.448896\tval-merror:0.518574\n",
      "[26]\ttrain-merror:0.449513\tval-merror:0.524517\n",
      "[27]\ttrain-merror:0.44989\tval-merror:0.521545\n",
      "[28]\ttrain-merror:0.450199\tval-merror:0.524517\n",
      "[29]\ttrain-merror:0.450576\tval-merror:0.520059\n",
      "[30]\ttrain-merror:0.450884\tval-merror:0.523031\n",
      "[31]\ttrain-merror:0.450919\tval-merror:0.521545\n",
      "[32]\ttrain-merror:0.45109\tval-merror:0.521545\n",
      "[33]\ttrain-merror:0.450679\tval-merror:0.523031\n",
      "[34]\ttrain-merror:0.450439\tval-merror:0.526003\n",
      "[35]\ttrain-merror:0.450884\tval-merror:0.527489\n",
      "[36]\ttrain-merror:0.450953\tval-merror:0.523031\n",
      "[37]\ttrain-merror:0.451501\tval-merror:0.523031\n",
      "[38]\ttrain-merror:0.450713\tval-merror:0.527489\n",
      "[39]\ttrain-merror:0.450302\tval-merror:0.526003\n",
      "[40]\ttrain-merror:0.450096\tval-merror:0.521545\n",
      "[41]\ttrain-merror:0.450062\tval-merror:0.523031\n",
      "[42]\ttrain-merror:0.449445\tval-merror:0.523031\n",
      "[43]\ttrain-merror:0.44965\tval-merror:0.523031\n",
      "[44]\ttrain-merror:0.450027\tval-merror:0.526003\n",
      "[45]\ttrain-merror:0.450096\tval-merror:0.520059\n",
      "[46]\ttrain-merror:0.449993\tval-merror:0.526003\n",
      "[47]\ttrain-merror:0.449959\tval-merror:0.527489\n",
      "[48]\ttrain-merror:0.449959\tval-merror:0.518574\n",
      "[49]\ttrain-merror:0.450439\tval-merror:0.523031\n",
      "[50]\ttrain-merror:0.450336\tval-merror:0.518574\n",
      "[51]\ttrain-merror:0.450302\tval-merror:0.520059\n",
      "[52]\ttrain-merror:0.450404\tval-merror:0.520059\n",
      "[53]\ttrain-merror:0.450267\tval-merror:0.520059\n",
      "[54]\ttrain-merror:0.449993\tval-merror:0.521545\n",
      "[55]\ttrain-merror:0.450062\tval-merror:0.524517\n",
      "[56]\ttrain-merror:0.449719\tval-merror:0.520059\n",
      "[57]\ttrain-merror:0.449273\tval-merror:0.518574\n",
      "[58]\ttrain-merror:0.448965\tval-merror:0.517088\n",
      "[59]\ttrain-merror:0.448862\tval-merror:0.514116\n",
      "[60]\ttrain-merror:0.448999\tval-merror:0.514116\n",
      "[61]\ttrain-merror:0.448588\tval-merror:0.515602\n",
      "[62]\ttrain-merror:0.448554\tval-merror:0.514116\n",
      "[63]\ttrain-merror:0.448279\tval-merror:0.514116\n",
      "[64]\ttrain-merror:0.448348\tval-merror:0.515602\n",
      "[65]\ttrain-merror:0.448177\tval-merror:0.51263\n",
      "[66]\ttrain-merror:0.448245\tval-merror:0.511144\n",
      "[67]\ttrain-merror:0.448108\tval-merror:0.509658\n",
      "[68]\ttrain-merror:0.448142\tval-merror:0.511144\n",
      "[69]\ttrain-merror:0.448382\tval-merror:0.511144\n",
      "[70]\ttrain-merror:0.447902\tval-merror:0.511144\n",
      "[71]\ttrain-merror:0.447765\tval-merror:0.51263\n",
      "[72]\ttrain-merror:0.447697\tval-merror:0.511144\n",
      "[73]\ttrain-merror:0.447491\tval-merror:0.511144\n",
      "[74]\ttrain-merror:0.447423\tval-merror:0.511144\n",
      "[75]\ttrain-merror:0.447011\tval-merror:0.514116\n",
      "[76]\ttrain-merror:0.446566\tval-merror:0.514116\n",
      "[77]\ttrain-merror:0.446531\tval-merror:0.517088\n",
      "[78]\ttrain-merror:0.446463\tval-merror:0.515602\n",
      "[79]\ttrain-merror:0.446908\tval-merror:0.514116\n",
      "[80]\ttrain-merror:0.446497\tval-merror:0.514116\n",
      "[81]\ttrain-merror:0.446634\tval-merror:0.514116\n",
      "[82]\ttrain-merror:0.44636\tval-merror:0.514116\n",
      "[83]\ttrain-merror:0.445983\tval-merror:0.51263\n",
      "[84]\ttrain-merror:0.445983\tval-merror:0.511144\n",
      "[85]\ttrain-merror:0.445606\tval-merror:0.511144\n",
      "[86]\ttrain-merror:0.445503\tval-merror:0.517088\n",
      "[87]\ttrain-merror:0.445263\tval-merror:0.515602\n",
      "[88]\ttrain-merror:0.445332\tval-merror:0.517088\n",
      "[89]\ttrain-merror:0.44516\tval-merror:0.517088\n",
      "[90]\ttrain-merror:0.445023\tval-merror:0.517088\n",
      "[91]\ttrain-merror:0.445058\tval-merror:0.517088\n",
      "[92]\ttrain-merror:0.445126\tval-merror:0.515602\n",
      "[93]\ttrain-merror:0.44516\tval-merror:0.515602\n",
      "[94]\ttrain-merror:0.445332\tval-merror:0.517088\n",
      "[95]\ttrain-merror:0.445366\tval-merror:0.520059\n",
      "[96]\ttrain-merror:0.444955\tval-merror:0.521545\n",
      "[97]\ttrain-merror:0.444475\tval-merror:0.515602\n",
      "[98]\ttrain-merror:0.444372\tval-merror:0.517088\n",
      "[99]\ttrain-merror:0.444132\tval-merror:0.518574\n",
      "[100]\ttrain-merror:0.444304\tval-merror:0.518574\n",
      "[101]\ttrain-merror:0.444064\tval-merror:0.518574\n",
      "[102]\ttrain-merror:0.443961\tval-merror:0.517088\n",
      "[103]\ttrain-merror:0.443687\tval-merror:0.514116\n",
      "[104]\ttrain-merror:0.443378\tval-merror:0.511144\n",
      "[105]\ttrain-merror:0.443447\tval-merror:0.51263\n",
      "[106]\ttrain-merror:0.443481\tval-merror:0.509658\n",
      "[107]\ttrain-merror:0.443721\tval-merror:0.51263\n",
      "[108]\ttrain-merror:0.443755\tval-merror:0.514116\n",
      "[109]\ttrain-merror:0.443858\tval-merror:0.51263\n",
      "[110]\ttrain-merror:0.44331\tval-merror:0.51263\n",
      "[111]\ttrain-merror:0.443378\tval-merror:0.51263\n",
      "[112]\ttrain-merror:0.443481\tval-merror:0.515602\n",
      "[113]\ttrain-merror:0.443549\tval-merror:0.517088\n",
      "[114]\ttrain-merror:0.443378\tval-merror:0.515602\n",
      "[115]\ttrain-merror:0.443447\tval-merror:0.515602\n",
      "[116]\ttrain-merror:0.443104\tval-merror:0.515602\n",
      "[117]\ttrain-merror:0.442864\tval-merror:0.515602\n",
      "[118]\ttrain-merror:0.442898\tval-merror:0.517088\n",
      "[119]\ttrain-merror:0.443001\tval-merror:0.51263\n",
      "[120]\ttrain-merror:0.442933\tval-merror:0.514116\n",
      "[121]\ttrain-merror:0.442693\tval-merror:0.517088\n",
      "[122]\ttrain-merror:0.442418\tval-merror:0.520059\n",
      "[123]\ttrain-merror:0.442179\tval-merror:0.517088\n",
      "[124]\ttrain-merror:0.442418\tval-merror:0.517088\n",
      "[125]\ttrain-merror:0.442247\tval-merror:0.518574\n",
      "[126]\ttrain-merror:0.442076\tval-merror:0.520059\n",
      "[127]\ttrain-merror:0.442213\tval-merror:0.518574\n",
      "[128]\ttrain-merror:0.442281\tval-merror:0.518574\n",
      "[129]\ttrain-merror:0.442179\tval-merror:0.515602\n",
      "[130]\ttrain-merror:0.442076\tval-merror:0.517088\n",
      "[131]\ttrain-merror:0.441939\tval-merror:0.515602\n",
      "[132]\ttrain-merror:0.442007\tval-merror:0.515602\n",
      "[133]\ttrain-merror:0.442041\tval-merror:0.515602\n",
      "[134]\ttrain-merror:0.441973\tval-merror:0.515602\n",
      "[135]\ttrain-merror:0.441973\tval-merror:0.515602\n",
      "[136]\ttrain-merror:0.441801\tval-merror:0.515602\n",
      "[137]\ttrain-merror:0.441904\tval-merror:0.514116\n",
      "[138]\ttrain-merror:0.441287\tval-merror:0.514116\n",
      "[139]\ttrain-merror:0.441013\tval-merror:0.517088\n",
      "[140]\ttrain-merror:0.441013\tval-merror:0.517088\n",
      "[141]\ttrain-merror:0.441047\tval-merror:0.517088\n",
      "[142]\ttrain-merror:0.440808\tval-merror:0.515602\n",
      "[143]\ttrain-merror:0.440636\tval-merror:0.515602\n",
      "[144]\ttrain-merror:0.440568\tval-merror:0.515602\n",
      "[145]\ttrain-merror:0.440465\tval-merror:0.515602\n",
      "[146]\ttrain-merror:0.440773\tval-merror:0.515602\n",
      "[147]\ttrain-merror:0.440602\tval-merror:0.515602\n",
      "[148]\ttrain-merror:0.440191\tval-merror:0.517088\n",
      "[149]\ttrain-merror:0.440259\tval-merror:0.517088\n",
      "[150]\ttrain-merror:0.439985\tval-merror:0.518574\n",
      "[151]\ttrain-merror:0.439848\tval-merror:0.518574\n",
      "[152]\ttrain-merror:0.439471\tval-merror:0.518574\n",
      "[153]\ttrain-merror:0.439231\tval-merror:0.518574\n",
      "[154]\ttrain-merror:0.439437\tval-merror:0.518574\n",
      "[155]\ttrain-merror:0.439505\tval-merror:0.518574\n",
      "[156]\ttrain-merror:0.439402\tval-merror:0.518574\n",
      "[157]\ttrain-merror:0.439368\tval-merror:0.517088\n",
      "[158]\ttrain-merror:0.439059\tval-merror:0.518574\n",
      "[159]\ttrain-merror:0.439059\tval-merror:0.517088\n",
      "[160]\ttrain-merror:0.438648\tval-merror:0.518574\n",
      "[161]\ttrain-merror:0.438648\tval-merror:0.520059\n",
      "[162]\ttrain-merror:0.438511\tval-merror:0.518574\n",
      "[163]\ttrain-merror:0.438511\tval-merror:0.517088\n",
      "[164]\ttrain-merror:0.438477\tval-merror:0.518574\n",
      "[165]\ttrain-merror:0.438614\tval-merror:0.515602\n",
      "[166]\ttrain-merror:0.438511\tval-merror:0.515602\n",
      "[167]\ttrain-merror:0.438682\tval-merror:0.517088\n",
      "Stopping. Best iteration:\n",
      "[67]\ttrain-merror:0.448108\tval-merror:0.509658\n",
      "\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "x_columns = [\n",
    "#     'win_bet_return', 'draw_bet_return', 'lose_bet_return', \n",
    "#     'year', 'month',\n",
    "#     'home_team_encoder', 'visit_team_encoder', 'game_encoder',\n",
    "    \n",
    "#     'h_avg_abs_gs',\n",
    "#     'h_avg_abs_gd', 'v_avg_abs_gs', 'v_avg_abs_gd', 'h_avg_abs_win',\n",
    "#     'h_avg_abs_draw', 'h_avg_abs_lose', 'v_avg_abs_win',\n",
    "#     'v_avg_abs_draw', 'v_avg_abs_lose',\n",
    "    \n",
    "#     'h_0_1_gd', \n",
    "#     'h_0_1_gs', \n",
    "#     'h_0_gd', \n",
    "#     'h_0_gs', \n",
    "#     'h_1_gd', \n",
    "#     'h_1_gs',\n",
    "#     'h_2_3_gd', \n",
    "#     'h_2_3_gs', \n",
    "#     'h_2_gd', 'h_2_gs', 'h_3_gd', 'h_3_gs',\n",
    "#     'h_4_gd', 'h_4_gs', 'h_5_gd', 'h_5_gs', 'h_6_gd', 'h_6_gs',\n",
    "#     'h_7_gd', 'h_7_gs', 'h_ab_4_gd', 'h_ab_4_gs', 'h_abs_avg_g',\n",
    "#     'h_abs_avg_gd', 'h_abs_avg_gs', 'h_abs_draw', 'h_abs_g',\n",
    "#     'h_abs_gd', 'h_abs_gs', 'h_abs_lose', 'h_abs_win', 'h_count',\n",
    "#     'h_host_count', 'h_host_draw', 'h_host_g', 'h_host_gd',\n",
    "#     'h_host_gs', 'h_host_lose', 'h_host_win', \n",
    "    \n",
    "#     'v_0_1_gd',\n",
    "#     'v_0_1_gs', 'v_0_gd', 'v_0_gs', 'v_1_gd', 'v_1_gs', 'v_2_3_gd',\n",
    "#     'v_2_3_gs', 'v_2_gd', 'v_2_gs', 'v_3_gd', 'v_3_gs', 'v_4_gd',\n",
    "#     'v_4_gs', 'v_5_gd', 'v_5_gs', 'v_6_gd', 'v_6_gs', 'v_7_gd',\n",
    "#     'v_7_gs', 'v_ab_4_gd', 'v_ab_4_gs', 'v_abs_avg_g', 'v_abs_avg_gd',\n",
    "#     'v_abs_avg_gs', 'v_abs_draw', 'v_abs_g', 'v_abs_gd', 'v_abs_gs',\n",
    "#     'v_abs_lose', 'v_abs_win', \n",
    "    \n",
    "#     'v_count', \n",
    "#     'v_visit_count',\n",
    "#     'v_visit_draw', 'v_visit_g', 'v_visit_gd', 'v_visit_gs',\n",
    "#     'v_visit_lose', 'v_visit_win',\n",
    "    \n",
    "#     'h_0_1_gd_rate', 'h_0_1_gs_rate',\n",
    "#     'h_0_gd_rate', 'h_0_gs_rate', 'h_1_gd_rate', 'h_1_gs_rate',\n",
    "#     'h_2_3_gd_rate', 'h_2_3_gs_rate', 'h_2_gd_rate', 'h_2_gs_rate',\n",
    "#     'h_3_gd_rate', 'h_3_gs_rate', 'h_4_gd_rate', 'h_4_gs_rate',\n",
    "#     'h_5_gd_rate', 'h_5_gs_rate', 'h_6_gd_rate', 'h_6_gs_rate',\n",
    "#     'h_7_gd_rate', 'h_7_gs_rate', 'h_ab_4_gd_rate', 'h_ab_4_gs_rate',\n",
    "#     'h_abs_draw_rate', 'h_abs_lose_rate', 'h_abs_win_rate',\n",
    "#     'v_0_1_gd_rate', 'v_0_1_gs_rate', 'v_0_gd_rate', 'v_0_gs_rate',\n",
    "#     'v_1_gd_rate', 'v_1_gs_rate', 'v_2_3_gd_rate', 'v_2_3_gs_rate',\n",
    "#     'v_2_gd_rate', 'v_2_gs_rate', 'v_3_gd_rate', 'v_3_gs_rate',\n",
    "#     'v_4_gd_rate', 'v_4_gs_rate', 'v_5_gd_rate', 'v_5_gs_rate',\n",
    "#     'v_6_gd_rate', 'v_6_gs_rate', 'v_7_gd_rate', 'v_7_gs_rate',\n",
    "    \n",
    "#     'v_ab_4_gd_rate', \n",
    "#     'v_ab_4_gs_rate', \n",
    "#     'v_abs_draw_rate',\n",
    "#     'v_abs_lose_rate', \n",
    "#     'v_abs_win_rate',\n",
    "#     'h_host_draw_rate',\n",
    "#     'h_host_g_rate', \n",
    "#     'h_host_gd_rate', \n",
    "#     'h_host_gs_rate',\n",
    "#     'h_host_lose_rate', \n",
    "#     'h_host_win_rate', \n",
    "#     'v_visit_draw_rate',\n",
    "#     'v_visit_g_rate', \n",
    "#     'v_visit_gd_rate', \n",
    "#     'v_visit_gs_rate',\n",
    "#     'v_visit_lose_rate', \n",
    "#     'v_visit_win_rate',\n",
    "    \n",
    "    'avg_init_draw_odd',\n",
    "    'avg_init_lose_odd', 'avg_init_win_odd', 'avg_new_draw_kelly',\n",
    "    'avg_new_draw_odd', 'avg_new_draw_rate', 'avg_new_lose_kelly',\n",
    "    'avg_new_lose_odd', 'avg_new_lose_rate', 'avg_new_win_kelly',\n",
    "    'avg_new_win_odd', 'avg_new_win_rate', 'avg_pay_rate',\n",
    "    'dispersion_draw', 'dispersion_lose', 'dispersion_win', 'id_y',\n",
    "    'max_init_draw_odd', 'max_init_lose_odd', 'max_init_win_odd',\n",
    "    'max_new_draw_kelly', 'max_new_draw_odd', 'max_new_draw_rate',\n",
    "    'max_new_lose_kelly', 'max_new_lose_odd', 'max_new_lose_rate',\n",
    "    'max_new_win_kelly', 'max_new_win_odd', 'max_new_win_rate',\n",
    "    'max_pay_rate', 'min_init_draw_odd', 'min_init_lose_odd',\n",
    "    'min_init_win_odd', 'min_new_draw_kelly', 'min_new_draw_odd',\n",
    "    'min_new_draw_rate', 'min_new_lose_kelly', 'min_new_lose_odd',\n",
    "    'min_new_lose_rate', 'min_new_win_kelly', 'min_new_win_odd',\n",
    "    'min_new_win_rate', 'min_pay_rate', 'std_draw', 'std_lose',\n",
    "    'std_win'\n",
    "]\n",
    "    \n",
    "params={\n",
    "    'booster':'gbtree',\n",
    "    # 这里手写数字是0-9，是一个多类的问题，因此采用了multisoft多分类器，\n",
    "    'objective': 'multi:softmax', \n",
    "#     'objective': 'multi:softprob',\n",
    "    'num_class':3, # 类数，与 multisoftmax 并用\n",
    "    \n",
    "    'gamma':0.01,  # 在树的叶子节点下一个分区的最小损失，越大算法模型越保守 。[0:]\n",
    "    \n",
    "    'max_depth':8, # 构建树的深度 [1:]\n",
    "    \n",
    "    #'lambda':450,  # L2 正则项权重\n",
    "    'subsample':0.7, # 采样训练数据，设置为0.5，随机选择一般的数据实例 (0:1]\n",
    "    'colsample_bytree':0.7, # 构建树树时的采样比率 (0:1]\n",
    "    #'min_child_weight':12, # 节点的最少特征数\n",
    "    'silent':1 ,\n",
    "    \n",
    "#     这部分需要调整\n",
    "#     'eta': 0.05, # 如同学习率\n",
    "    'eta': 0.01, # 如同学习率\n",
    "    \n",
    "    \n",
    "    'seed':2018,\n",
    "    'nthread':4,# cpu 线程数,根据自己U的个数适当调整\n",
    "}\n",
    "\n",
    "# t = train_dataset_df[\n",
    "#     (train_dataset_df['win_bet_return'] <= 2) |\n",
    "#     (train_dataset_df['draw_bet_return'] <= 2) |\n",
    "#     (train_dataset_df['lose_bet_return'] <= 2)\n",
    "# ]\n",
    "\n",
    "t = train_dataset_df\n",
    "\n",
    "print(len(t))\n",
    "\n",
    "train_dataset = t[t['year'] < 2019]\n",
    "test_dataset = t[t['year'] == 2019]\n",
    "\n",
    "valid_dataset = test_dataset[test_dataset['month'] < 3]\n",
    "test_dataset = test_dataset[test_dataset['month'] >= 3]\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_dataset[x_columns], label=train_dataset['fix_result'])\n",
    "xgtest = xgb.DMatrix(test_dataset[x_columns], label=test_dataset['fix_result'])\n",
    "xgvalid = xgb.DMatrix(valid_dataset[x_columns], label=valid_dataset['fix_result'])\n",
    "\n",
    "watchlist = [(xgtrain, 'train'),(xgvalid, 'val')]\n",
    "\n",
    "num_rounds = 10000\n",
    "stop_rounds = 100\n",
    "\n",
    "# num_rounds = 10000\n",
    "# stop_rounds = 300\n",
    "\n",
    "\n",
    "model = xgb.train(params, xgtrain, num_rounds, watchlist,early_stopping_rounds=stop_rounds)\n",
    "print(model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3452244 , 0.3264327 , 0.3283429 ],\n",
       "       [0.3335006 , 0.3313826 , 0.33511677],\n",
       "       [0.32226524, 0.32644787, 0.35128686],\n",
       "       ...,\n",
       "       [0.32169273, 0.32734713, 0.3509601 ],\n",
       "       [0.32342005, 0.32687503, 0.34970492],\n",
       "       [0.34933007, 0.32826293, 0.32240704]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求可能性的时候用：'objective': 'multi:softprob',\n",
    "\n",
    "pred_probs = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['fix_result'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(items):\n",
    "    if items[0] <= items[1] and items[0] <= items[2]:\n",
    "        return [1,2]\n",
    "    elif items[1] <= items[0] and items[1] <= items[2]:\n",
    "        return [0,2]\n",
    "    elif items[2] <= items[0] and items[2] <= items[1]:\n",
    "        return [0,1]\n",
    "    \n",
    "fix_results = test_dataset['fix_result'].values\n",
    "\n",
    "results = []\n",
    "for i in range(len(pred_probs)):\n",
    "    items = pred_probs[i]\n",
    "    probs = get_result(items)\n",
    "    \n",
    "    if fix_results[i] in probs:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7400530503978779"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(results).sum() / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4880636604774536"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "preds\n",
    "\n",
    "accuracy_score(test_dataset['fix_result'], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM多分类训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leewind/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/leewind/.local/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/home/leewind/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "# Seperating Predictors and Outcome values from train and test sets\n",
    "X_train = train_dataset[x_columns]\n",
    "Y_train_label = train_dataset['fix_result'].values.astype(object)\n",
    "\n",
    "X_test = test_dataset[x_columns]\n",
    "Y_test_label = test_dataset['fix_result'].values.astype(object)\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# encoding train labels \n",
    "encoder.fit(Y_train_label)\n",
    "Y_train = encoder.transform(Y_train_label)\n",
    "\n",
    "# encoding test labels \n",
    "encoder.fit(Y_test_label)\n",
    "Y_test = encoder.transform(Y_test_label)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "#                      'C': [1, 10, 100, 1000]},\n",
    "#                     {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "params_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [10, 100]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# svm_model = GridSearchCV(SVC(), params_grid, cv=5)\n",
    "# svm_model.fit(X_train_scaled, Y_train)\n",
    "final_model = SVC(C=1, kernel='rbf', degree=3, gamma='auto', verbose=True)\n",
    "final_model.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score for SVM: 0.509186\n",
      "Testing  set score for SVM: 0.503095\n"
     ]
    }
   ],
   "source": [
    "# final_model = svm_model.best_estimator_\n",
    "\n",
    "print(\"Training set score for SVM: %f\" % final_model.score(X_train_scaled , Y_train))\n",
    "print(\"Testing  set score for SVM: %f\" % final_model.score(X_test_scaled  , Y_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
