{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取让球的赔率信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host='localhost', user='root', password='breadt@2019', db='breadt-football-ml', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "with connection.cursor() as cursor:\n",
    "    sql = 'select * from `breadt_football_offset_info`;'\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    train_game_offset_df = pd.DataFrame(rows)\n",
    "    \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['两半/三', '半/一', '两球', '一球', '受平/半', '平/半', '两/两半', '半球', '球半/两',\n",
       "       '球半', '受半球', '受半/一', '受一/球半', '受一球', '一/球半', '平手', '受球半', '受球半/两',\n",
       "       '受两球', '三球', '两半', '受两/两半', '受三球', '三/三半', '受三/三半', '六球', '四球',\n",
       "       '四半', '受两半', '八半', '受两半/三', '三半/四', '三半', '受六球', '受四半/五', '受五半/六',\n",
       "       '五球', '受三半', '四/四半', '五半', '受四/四半', '四半/五', '受六半', '六半/七', '七半',\n",
       "       '受四半', '受五/五半', '受六/六半', '受四球', '六半', '受三半/四', '七/七半', '受五半'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_game_offset_df['new_offset'].drop_duplicates().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_value_from_work(row):\n",
    "    map = {\n",
    "        '两半/三': -2.75, \n",
    "        '半/一': -0.75, \n",
    "        '两球': -2, \n",
    "        '一球': -1, \n",
    "        '受平/半': 0.25, \n",
    "        '平/半': -0.25, \n",
    "        '两/两半': -2.25, \n",
    "        '半球': -0.5,\n",
    "        '球半/两': -1.75,\n",
    "        '球半': -1.5,\n",
    "        '受半球': 0.5, \n",
    "        '受半/一': 0.75, \n",
    "        '受一/球半': 1.25, \n",
    "        '受一球': 1, \n",
    "        '一/球半': -1.25, \n",
    "        '平手': 0, \n",
    "        '受球半': 1.5, \n",
    "        '受球半/两': 1.75,\n",
    "        '受两球': 2, \n",
    "        '三球': -3, \n",
    "        '两半': -2.5, \n",
    "        '受两/两半': 2.25, \n",
    "        '受三球': 3, \n",
    "        '三/三半': -3.25, \n",
    "        '受三/三半': 3.25, \n",
    "        '六球': -6, \n",
    "        '四球': -4,\n",
    "        '四半': -4.5, \n",
    "        '受两半': 2.5, \n",
    "        '八半': -8.5, \n",
    "        '受两半/三': 2.75, \n",
    "        '三半/四': -3.75, \n",
    "        '三半': -3.5, \n",
    "        '受六球': 6, \n",
    "        '受四半/五': 4.75, \n",
    "        '受五半/六': 5.75,\n",
    "        '五球': -5, \n",
    "        '受三半': 3.5, \n",
    "        '四/四半': -4.25, \n",
    "        '五半': -5.5, \n",
    "        '受四/四半': 4.25, \n",
    "        '四半/五': -4.75, \n",
    "        '受六半': 6.5, \n",
    "        '六半/七': -6.75, \n",
    "        '七半': -7.5,\n",
    "        '受四半': 4.5, \n",
    "        '受五/五半': 5.25, \n",
    "        '受六/六半': 6.25, \n",
    "        '受四球': 4, \n",
    "        '六半': -6.5,\n",
    "        '受三半/四': 3.75, \n",
    "        '七/七半': -7.25,\n",
    "        '受五半': 5.5\n",
    "    }\n",
    "    \n",
    "    v = map[row.new_offset]\n",
    "    \n",
    "    # 原来只需要返回v这里，我特别做处理\n",
    "#     if v is not None:\n",
    "#         num = v/0.25\n",
    "#         if num % 4 > 0:\n",
    "#             if num > 0:\n",
    "#                 if num % 4 == 3:\n",
    "#                     return (num + 1)*0.25\n",
    "#                 elif num % 4 == 2\n",
    "#                 return (num - 1)*0.25\n",
    "#             else:\n",
    "#                 return (num + 1)*0.25\n",
    "    \n",
    "    return v\n",
    "\n",
    "train_game_offset_df['new_offset_val'] = train_game_offset_df.apply(lambda row: take_value_from_work(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>id</th>\n",
       "      <th>init_host</th>\n",
       "      <th>init_offset</th>\n",
       "      <th>init_visit</th>\n",
       "      <th>matchid</th>\n",
       "      <th>new_host</th>\n",
       "      <th>new_host_rate</th>\n",
       "      <th>new_offset</th>\n",
       "      <th>new_visit</th>\n",
       "      <th>new_visit_rate</th>\n",
       "      <th>new_offset_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>澳门</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>三/三半</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1235399</td>\n",
       "      <td>0.90</td>\n",
       "      <td>50.26</td>\n",
       "      <td>两半/三</td>\n",
       "      <td>0.92</td>\n",
       "      <td>49.74</td>\n",
       "      <td>-2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>澳门</td>\n",
       "      <td>2</td>\n",
       "      <td>1.06</td>\n",
       "      <td>半/一</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1765346</td>\n",
       "      <td>0.86</td>\n",
       "      <td>50.53</td>\n",
       "      <td>半/一</td>\n",
       "      <td>0.90</td>\n",
       "      <td>49.47</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>澳门</td>\n",
       "      <td>3</td>\n",
       "      <td>0.82</td>\n",
       "      <td>两球</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1765347</td>\n",
       "      <td>0.81</td>\n",
       "      <td>51.86</td>\n",
       "      <td>两球</td>\n",
       "      <td>0.95</td>\n",
       "      <td>48.14</td>\n",
       "      <td>-2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>澳门</td>\n",
       "      <td>4</td>\n",
       "      <td>0.76</td>\n",
       "      <td>一球</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1765348</td>\n",
       "      <td>0.88</td>\n",
       "      <td>50.00</td>\n",
       "      <td>一球</td>\n",
       "      <td>0.88</td>\n",
       "      <td>50.00</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>澳门</td>\n",
       "      <td>5</td>\n",
       "      <td>0.70</td>\n",
       "      <td>受平/半</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1765349</td>\n",
       "      <td>0.72</td>\n",
       "      <td>54.26</td>\n",
       "      <td>受平/半</td>\n",
       "      <td>1.04</td>\n",
       "      <td>45.74</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>澳门</td>\n",
       "      <td>6</td>\n",
       "      <td>0.80</td>\n",
       "      <td>平/半</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1765350</td>\n",
       "      <td>0.96</td>\n",
       "      <td>47.87</td>\n",
       "      <td>平/半</td>\n",
       "      <td>0.80</td>\n",
       "      <td>52.13</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>澳门</td>\n",
       "      <td>7</td>\n",
       "      <td>0.68</td>\n",
       "      <td>两/两半</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1765351</td>\n",
       "      <td>0.74</td>\n",
       "      <td>53.72</td>\n",
       "      <td>两/两半</td>\n",
       "      <td>1.02</td>\n",
       "      <td>46.28</td>\n",
       "      <td>-2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>澳门</td>\n",
       "      <td>8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>平/半</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1765352</td>\n",
       "      <td>0.86</td>\n",
       "      <td>50.53</td>\n",
       "      <td>半球</td>\n",
       "      <td>0.90</td>\n",
       "      <td>49.47</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>澳门</td>\n",
       "      <td>9</td>\n",
       "      <td>0.98</td>\n",
       "      <td>半球</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1765354</td>\n",
       "      <td>1.16</td>\n",
       "      <td>42.55</td>\n",
       "      <td>半球</td>\n",
       "      <td>0.60</td>\n",
       "      <td>57.45</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>澳门</td>\n",
       "      <td>10</td>\n",
       "      <td>0.90</td>\n",
       "      <td>两/两半</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1765358</td>\n",
       "      <td>1.06</td>\n",
       "      <td>46.07</td>\n",
       "      <td>两球</td>\n",
       "      <td>0.76</td>\n",
       "      <td>53.93</td>\n",
       "      <td>-2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>澳门</td>\n",
       "      <td>11</td>\n",
       "      <td>0.84</td>\n",
       "      <td>球半/两</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1765886</td>\n",
       "      <td>1.06</td>\n",
       "      <td>46.63</td>\n",
       "      <td>球半/两</td>\n",
       "      <td>0.80</td>\n",
       "      <td>53.37</td>\n",
       "      <td>-1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>澳门</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>平/半</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1765887</td>\n",
       "      <td>0.98</td>\n",
       "      <td>48.70</td>\n",
       "      <td>平/半</td>\n",
       "      <td>0.88</td>\n",
       "      <td>51.30</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>澳门</td>\n",
       "      <td>13</td>\n",
       "      <td>1.08</td>\n",
       "      <td>平/半</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1765888</td>\n",
       "      <td>1.00</td>\n",
       "      <td>48.19</td>\n",
       "      <td>平/半</td>\n",
       "      <td>0.86</td>\n",
       "      <td>51.81</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>澳门</td>\n",
       "      <td>14</td>\n",
       "      <td>1.06</td>\n",
       "      <td>球半/两</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1765889</td>\n",
       "      <td>0.91</td>\n",
       "      <td>50.52</td>\n",
       "      <td>球半</td>\n",
       "      <td>0.95</td>\n",
       "      <td>49.48</td>\n",
       "      <td>-1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>澳门</td>\n",
       "      <td>15</td>\n",
       "      <td>0.92</td>\n",
       "      <td>半/一</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1765890</td>\n",
       "      <td>0.72</td>\n",
       "      <td>55.44</td>\n",
       "      <td>半/一</td>\n",
       "      <td>1.14</td>\n",
       "      <td>44.56</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>澳门</td>\n",
       "      <td>16</td>\n",
       "      <td>0.92</td>\n",
       "      <td>受半球</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1765891</td>\n",
       "      <td>0.86</td>\n",
       "      <td>51.81</td>\n",
       "      <td>受半球</td>\n",
       "      <td>1.00</td>\n",
       "      <td>48.19</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>澳门</td>\n",
       "      <td>17</td>\n",
       "      <td>0.86</td>\n",
       "      <td>受半球</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1765892</td>\n",
       "      <td>1.05</td>\n",
       "      <td>46.89</td>\n",
       "      <td>受半球</td>\n",
       "      <td>0.81</td>\n",
       "      <td>53.11</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>澳门</td>\n",
       "      <td>18</td>\n",
       "      <td>0.80</td>\n",
       "      <td>平/半</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1765893</td>\n",
       "      <td>0.80</td>\n",
       "      <td>53.37</td>\n",
       "      <td>平/半</td>\n",
       "      <td>1.06</td>\n",
       "      <td>46.63</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>澳门</td>\n",
       "      <td>19</td>\n",
       "      <td>0.88</td>\n",
       "      <td>受一球</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1765894</td>\n",
       "      <td>1.03</td>\n",
       "      <td>47.41</td>\n",
       "      <td>受半/一</td>\n",
       "      <td>0.83</td>\n",
       "      <td>52.59</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>澳门</td>\n",
       "      <td>20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>平/半</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1765895</td>\n",
       "      <td>0.96</td>\n",
       "      <td>49.22</td>\n",
       "      <td>平/半</td>\n",
       "      <td>0.90</td>\n",
       "      <td>50.78</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company  id  init_host init_offset  init_visit  matchid  new_host  \\\n",
       "0       澳门   1       0.82        三/三半        1.00  1235399      0.90   \n",
       "1       澳门   2       1.06         半/一        0.70  1765346      0.86   \n",
       "2       澳门   3       0.82          两球        0.94  1765347      0.81   \n",
       "3       澳门   4       0.76          一球        1.00  1765348      0.88   \n",
       "4       澳门   5       0.70        受平/半        1.06  1765349      0.72   \n",
       "5       澳门   6       0.80         平/半        0.96  1765350      0.96   \n",
       "6       澳门   7       0.68        两/两半        1.08  1765351      0.74   \n",
       "7       澳门   8       0.76         平/半        1.00  1765352      0.86   \n",
       "8       澳门   9       0.98          半球        0.78  1765354      1.16   \n",
       "9       澳门  10       0.90        两/两半        0.92  1765358      1.06   \n",
       "10      澳门  11       0.84        球半/两        1.02  1765886      1.06   \n",
       "11      澳门  12       1.00         平/半        0.86  1765887      0.98   \n",
       "12      澳门  13       1.08         平/半        0.78  1765888      1.00   \n",
       "13      澳门  14       1.06        球半/两        0.80  1765889      0.91   \n",
       "14      澳门  15       0.92         半/一        0.94  1765890      0.72   \n",
       "15      澳门  16       0.92         受半球        0.94  1765891      0.86   \n",
       "16      澳门  17       0.86         受半球        1.00  1765892      1.05   \n",
       "17      澳门  18       0.80         平/半        1.06  1765893      0.80   \n",
       "18      澳门  19       0.88         受一球        0.98  1765894      1.03   \n",
       "19      澳门  20       1.08         平/半        0.78  1765895      0.96   \n",
       "\n",
       "    new_host_rate new_offset  new_visit  new_visit_rate  new_offset_val  \n",
       "0           50.26       两半/三       0.92           49.74           -2.75  \n",
       "1           50.53        半/一       0.90           49.47           -0.75  \n",
       "2           51.86         两球       0.95           48.14           -2.00  \n",
       "3           50.00         一球       0.88           50.00           -1.00  \n",
       "4           54.26       受平/半       1.04           45.74            0.25  \n",
       "5           47.87        平/半       0.80           52.13           -0.25  \n",
       "6           53.72       两/两半       1.02           46.28           -2.25  \n",
       "7           50.53         半球       0.90           49.47           -0.50  \n",
       "8           42.55         半球       0.60           57.45           -0.50  \n",
       "9           46.07         两球       0.76           53.93           -2.00  \n",
       "10          46.63       球半/两       0.80           53.37           -1.75  \n",
       "11          48.70        平/半       0.88           51.30           -0.25  \n",
       "12          48.19        平/半       0.86           51.81           -0.25  \n",
       "13          50.52         球半       0.95           49.48           -1.50  \n",
       "14          55.44        半/一       1.14           44.56           -0.75  \n",
       "15          51.81        受半球       1.00           48.19            0.50  \n",
       "16          46.89        受半球       0.81           53.11            0.50  \n",
       "17          53.37        平/半       1.06           46.63           -0.25  \n",
       "18          47.41       受半/一       0.83           52.59            0.75  \n",
       "19          49.22        平/半       0.90           50.78           -0.25  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_game_offset_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取全量的竞彩比赛列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host='localhost', user='root', password='breadt@2019', db='breadt-football-ml', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "with connection.cursor() as cursor:\n",
    "    sql = 'select * from `breadt_football_game_list`;'\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    train_game_list_df = pd.DataFrame(rows)\n",
    "    \n",
    "    connection.close()\n",
    "\n",
    "train_game_list_df['source'] = 'jc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取全量的胜负彩比赛列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host='localhost', user='root', password='breadt@2019', db='breadt-football-ml', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "with connection.cursor() as cursor:\n",
    "    sql = 'select * from `breadt_lottery_info`;'\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    train_lottery_game_list_df = pd.DataFrame(rows)\n",
    "    \n",
    "    connection.close()\n",
    "\n",
    "train_lottery_game_list_df['source'] = 'lottery'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并竞彩比赛列表和胜负彩比赛列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train_lottery_game_list_df.drop(['issue'], axis=1)\n",
    "df = pd.concat([train_game_list_df, tmp])\n",
    "df = df[['matchid', 'game', 'home_team', 'visit_team', 'gs', 'gd', 'gn', 'time', 'result', 'win_bet_return', 'draw_bet_return', 'lose_bet_return', 'source']]\n",
    "df = df.drop_duplicates(subset=['matchid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **设定训练范围** 并处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_group = ['澳超', '英超', '德甲', '德乙', '法甲', '西甲', '意甲', '日职', '英甲', '英冠', '苏超', '法乙', '葡超', '荷甲', '荷乙', '韩K联', '瑞典超', '挪超', '美职', '日乙', '俄超', '比甲', '瑞典甲', '法丙', '挪甲', '英乙', '苏冠', '巴甲', '智利甲', '墨超', '智利乙', '阿甲', '欧冠', '欧罗巴']\n",
    "match_group = ['澳超', '英超', '德甲', '德乙', '法甲', '西甲', '意甲', '日职', '英甲', '英冠', '苏超', '法乙', '葡超', '荷甲', '荷乙', '韩K联', '瑞典超', '挪超', '美职', '日乙', '俄超', '比甲', '瑞典甲', '法丙', '挪甲', '英乙', '苏冠', '巴甲', '智利甲', '墨超', '智利乙', '阿甲']\n",
    "match_df = df[(df['game'].isin(match_group))]\n",
    "match_df = match_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对team做encode 这个encoder后面预测的时候还会用到\n",
    "teams = list(set(df['home_team'].values) | set(df['visit_team'].values))\n",
    "team_encoder = preprocessing.LabelEncoder()\n",
    "team_encoder.fit(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_team(df):\n",
    "    df['home_team_encoder'] = team_encoder.transform(df['home_team'])\n",
    "    df['visit_team_encoder'] = team_encoder.transform(df['visit_team'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比赛名称encode\n",
    "games = list(set(match_df['game'].values))\n",
    "game_encoder = preprocessing.LabelEncoder()\n",
    "game_encoder.fit(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_game(df):\n",
    "    df['game_encoder'] = game_encoder.transform(df['game'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df['year'] = match_df.apply(lambda row: row.time.year, axis=1)\n",
    "match_df['month'] = match_df.apply(lambda row: row.time.month, axis=1)\n",
    "match_df['day'] = match_df.apply(lambda row: row.time.day, axis=1)\n",
    "match_df['fix_result'] = match_df.apply(lambda row: int(row.result) if row.result < 3 else 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = encode_team(match_df)\n",
    "match_df = encode_game(match_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 43477 entries, 1 to 12650\n",
      "Data columns (total 20 columns):\n",
      "matchid               43477 non-null int64\n",
      "game                  43477 non-null object\n",
      "home_team             43477 non-null object\n",
      "visit_team            43477 non-null object\n",
      "gs                    43477 non-null int64\n",
      "gd                    43477 non-null int64\n",
      "gn                    43477 non-null int64\n",
      "time                  43477 non-null datetime64[ns]\n",
      "result                43477 non-null int64\n",
      "win_bet_return        43477 non-null float64\n",
      "draw_bet_return       43477 non-null float64\n",
      "lose_bet_return       43477 non-null float64\n",
      "source                43477 non-null object\n",
      "year                  43477 non-null int64\n",
      "month                 43477 non-null int64\n",
      "day                   43477 non-null int64\n",
      "fix_result            43477 non-null int64\n",
      "home_team_encoder     43477 non-null int64\n",
      "visit_team_encoder    43477 non-null int64\n",
      "game_encoder          43477 non-null int64\n",
      "dtypes: datetime64[ns](1), float64(3), int64(12), object(4)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "match_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host='localhost', user='root', password='breadt@2019', db='breadt-football-ml', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "with connection.cursor() as cursor:\n",
    "    sql = 'select * from `breadt_football_recent_feature_info`;'\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    train_feature_df = pd.DataFrame(rows)\n",
    "    \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_goal_info(prefix, df):\n",
    "    target_cols = [\n",
    "        '_0_1_gd', \n",
    "        '_0_1_gs', \n",
    "        '_0_gd', \n",
    "        '_0_gs', \n",
    "        '_1_gd', \n",
    "        '_1_gs',\n",
    "        '_2_3_gd', \n",
    "        '_2_3_gs', \n",
    "        '_2_gd', \n",
    "        '_2_gs', \n",
    "        '_3_gd', \n",
    "        '_3_gs',\n",
    "        '_4_gd', \n",
    "        '_4_gs', \n",
    "        '_5_gd', \n",
    "        '_5_gs', \n",
    "        '_6_gd', \n",
    "        '_6_gs',\n",
    "        '_7_gd', \n",
    "        '_7_gs', \n",
    "        '_ab_4_gd', \n",
    "        '_ab_4_gs',\n",
    "        '_abs_draw', \n",
    "        '_abs_lose', \n",
    "        '_abs_win']\n",
    "\n",
    "    for k in target_cols:\n",
    "        df[prefix + k + '_rate'] = df[prefix + k] / df[prefix + '_count']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_goal_pref_info(prefix, df):\n",
    "    target_cols = [\n",
    "        '_draw', '_g', '_gd',\n",
    "        '_gs', '_lose', '_win',\n",
    "    ]\n",
    "\n",
    "    for k in target_cols:\n",
    "        df[prefix + k + '_rate'] = df[prefix + k] / df[prefix + '_count']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_df = take_goal_info('h', train_feature_df)\n",
    "train_feature_df = take_goal_info('v', train_feature_df)\n",
    "\n",
    "train_feature_df = take_goal_pref_info('h_host', train_feature_df)\n",
    "train_feature_df = take_goal_pref_info('v_visit', train_feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有球队的主客场进球平均数\n",
    "\n",
    "train_feature_df['h_avg_abs_gs'] = train_feature_df['h_abs_gs'].sum() / train_feature_df['h_count'].sum()\n",
    "train_feature_df['h_avg_abs_gd'] = train_feature_df['h_abs_gd'].sum() / train_feature_df['h_count'].sum()\n",
    "train_feature_df['v_avg_abs_gs'] = train_feature_df['v_abs_gs'].sum() / train_feature_df['v_count'].sum()\n",
    "train_feature_df['v_avg_abs_gd'] = train_feature_df['v_abs_gd'].sum() / train_feature_df['v_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_df['h_avg_abs_win'] = train_feature_df['h_abs_win'].sum() / train_feature_df['h_count'].sum()\n",
    "train_feature_df['h_avg_abs_draw'] = train_feature_df['h_abs_draw'].sum() / train_feature_df['h_count'].sum()\n",
    "train_feature_df['h_avg_abs_lose'] = train_feature_df['h_abs_lose'].sum() / train_feature_df['h_count'].sum()\n",
    "\n",
    "train_feature_df['v_avg_abs_win'] = train_feature_df['v_abs_win'].sum() / train_feature_df['v_count'].sum()\n",
    "train_feature_df['v_avg_abs_draw'] = train_feature_df['v_abs_draw'].sum() / train_feature_df['v_count'].sum()\n",
    "train_feature_df['v_avg_abs_lose'] = train_feature_df['v_abs_lose'].sum() / train_feature_df['v_count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取赔率信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host='localhost', user='root', password='breadt@2019', db='breadt-football-ml', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "with connection.cursor() as cursor:\n",
    "    sql = 'select * from `breadt_match_odd_info`;'\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    train_match_odd_df = pd.DataFrame(rows)\n",
    "    \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_init_draw_odd</th>\n",
       "      <th>avg_init_lose_odd</th>\n",
       "      <th>avg_init_win_odd</th>\n",
       "      <th>avg_new_draw_kelly</th>\n",
       "      <th>avg_new_draw_odd</th>\n",
       "      <th>avg_new_draw_rate</th>\n",
       "      <th>avg_new_lose_kelly</th>\n",
       "      <th>avg_new_lose_odd</th>\n",
       "      <th>avg_new_lose_rate</th>\n",
       "      <th>avg_new_win_kelly</th>\n",
       "      <th>...</th>\n",
       "      <th>min_new_lose_kelly</th>\n",
       "      <th>min_new_lose_odd</th>\n",
       "      <th>min_new_lose_rate</th>\n",
       "      <th>min_new_win_kelly</th>\n",
       "      <th>min_new_win_odd</th>\n",
       "      <th>min_new_win_rate</th>\n",
       "      <th>min_pay_rate</th>\n",
       "      <th>std_draw</th>\n",
       "      <th>std_lose</th>\n",
       "      <th>std_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.46</td>\n",
       "      <td>30.07</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>13.33</td>\n",
       "      <td>7.28</td>\n",
       "      <td>1.02</td>\n",
       "      <td>31.71</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.00</td>\n",
       "      <td>66.71</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1007.93</td>\n",
       "      <td>11029.60</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.38</td>\n",
       "      <td>4.29</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.43</td>\n",
       "      <td>26.55</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4.46</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.70</td>\n",
       "      <td>16.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.50</td>\n",
       "      <td>48.58</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.83</td>\n",
       "      <td>12.46</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.72</td>\n",
       "      <td>14.49</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.92</td>\n",
       "      <td>6.97</td>\n",
       "      <td>13.17</td>\n",
       "      <td>0.95</td>\n",
       "      <td>15.31</td>\n",
       "      <td>6.19</td>\n",
       "      <td>0.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.80</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.05</td>\n",
       "      <td>69.75</td>\n",
       "      <td>0.83</td>\n",
       "      <td>37.80</td>\n",
       "      <td>1120.81</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.02</td>\n",
       "      <td>17.87</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.92</td>\n",
       "      <td>7.89</td>\n",
       "      <td>11.68</td>\n",
       "      <td>0.94</td>\n",
       "      <td>17.85</td>\n",
       "      <td>5.29</td>\n",
       "      <td>0.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>10.50</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.02</td>\n",
       "      <td>75.49</td>\n",
       "      <td>0.83</td>\n",
       "      <td>93.17</td>\n",
       "      <td>1417.25</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.98</td>\n",
       "      <td>6.79</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4.06</td>\n",
       "      <td>22.43</td>\n",
       "      <td>0.92</td>\n",
       "      <td>7.17</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.69</td>\n",
       "      <td>5.35</td>\n",
       "      <td>9.54</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.30</td>\n",
       "      <td>59.57</td>\n",
       "      <td>0.83</td>\n",
       "      <td>4.23</td>\n",
       "      <td>90.73</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_init_draw_odd  avg_init_lose_odd  avg_init_win_odd  avg_new_draw_kelly  \\\n",
       "0              13.46              30.07              1.03                0.97   \n",
       "1               3.38               4.29              1.75                0.91   \n",
       "2               6.72              14.49              1.14                0.92   \n",
       "3               8.02              17.87              1.09                0.92   \n",
       "4               3.98               6.79              1.42                0.91   \n",
       "\n",
       "   avg_new_draw_odd  avg_new_draw_rate  avg_new_lose_kelly  avg_new_lose_odd  \\\n",
       "0             13.33               7.28                1.02             31.71   \n",
       "1              3.43              26.55                0.91              4.46   \n",
       "2              6.97              13.17                0.95             15.31   \n",
       "3              7.89              11.68                0.94             17.85   \n",
       "4              4.06              22.43                0.92              7.17   \n",
       "\n",
       "   avg_new_lose_rate  avg_new_win_kelly  ...  min_new_lose_kelly  \\\n",
       "0               3.21               0.92  ...                0.18   \n",
       "1              20.50               0.91  ...                0.76   \n",
       "2               6.19               0.91  ...                0.48   \n",
       "3               5.29               0.91  ...                0.56   \n",
       "4              12.88               0.91  ...                0.69   \n",
       "\n",
       "   min_new_lose_odd  min_new_lose_rate  min_new_win_kelly  min_new_win_odd  \\\n",
       "0              5.50               1.17               0.90             1.00   \n",
       "1              3.70              16.79               0.79             1.50   \n",
       "2              7.80               3.38               0.85             1.05   \n",
       "3             10.50               2.86               0.85             1.02   \n",
       "4              5.35               9.54               0.84             1.30   \n",
       "\n",
       "   min_new_win_rate  min_pay_rate  std_draw  std_lose  std_win  \n",
       "0             66.71          0.83   1007.93  11029.60     0.04  \n",
       "1             48.58          0.83      1.83     12.46     0.35  \n",
       "2             69.75          0.83     37.80   1120.81     0.05  \n",
       "3             75.49          0.83     93.17   1417.25     0.04  \n",
       "4             59.57          0.83      4.23     90.73     0.14  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_match_odd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31493 entries, 0 to 43278\n",
      "Columns: 217 entries, matchid to std_win\n",
      "dtypes: datetime64[ns](1), float64(200), int64(12), object(4)\n",
      "memory usage: 52.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train_dataset_df = pd.merge(match_df, train_feature_df, on='matchid', how='left')\n",
    "train_dataset_df = pd.merge(train_dataset_df, train_match_odd_df, on='matchid', how='left')\n",
    "train_dataset_df = train_dataset_df.dropna()\n",
    "train_dataset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['matchid', 'game', 'home_team', 'visit_team', 'gs', 'gd', 'gn',\n",
       "       'time', 'result', 'win_bet_return', 'draw_bet_return',\n",
       "       'lose_bet_return', 'source', 'year', 'month', 'day', 'fix_result',\n",
       "       'home_team_encoder', 'visit_team_encoder', 'game_encoder',\n",
       "       'h_0_1_gd', 'h_0_1_gs', 'h_0_gd', 'h_0_gs', 'h_1_gd', 'h_1_gs',\n",
       "       'h_2_3_gd', 'h_2_3_gs', 'h_2_gd', 'h_2_gs', 'h_3_gd', 'h_3_gs',\n",
       "       'h_4_gd', 'h_4_gs', 'h_5_gd', 'h_5_gs', 'h_6_gd', 'h_6_gs',\n",
       "       'h_7_gd', 'h_7_gs', 'h_ab_4_gd', 'h_ab_4_gs', 'h_abs_avg_g',\n",
       "       'h_abs_avg_gd', 'h_abs_avg_gs', 'h_abs_draw', 'h_abs_g',\n",
       "       'h_abs_gd', 'h_abs_gs', 'h_abs_lose', 'h_abs_win', 'h_count',\n",
       "       'h_host_count', 'h_host_draw', 'h_host_g', 'h_host_gd',\n",
       "       'h_host_gs', 'h_host_lose', 'h_host_win', 'id_x', 'v_0_1_gd',\n",
       "       'v_0_1_gs', 'v_0_gd', 'v_0_gs', 'v_1_gd', 'v_1_gs', 'v_2_3_gd',\n",
       "       'v_2_3_gs', 'v_2_gd', 'v_2_gs', 'v_3_gd', 'v_3_gs', 'v_4_gd',\n",
       "       'v_4_gs', 'v_5_gd', 'v_5_gs', 'v_6_gd', 'v_6_gs', 'v_7_gd',\n",
       "       'v_7_gs', 'v_ab_4_gd', 'v_ab_4_gs', 'v_abs_avg_g', 'v_abs_avg_gd',\n",
       "       'v_abs_avg_gs', 'v_abs_draw', 'v_abs_g', 'v_abs_gd', 'v_abs_gs',\n",
       "       'v_abs_lose', 'v_abs_win', 'v_count', 'v_visit_count',\n",
       "       'v_visit_draw', 'v_visit_g', 'v_visit_gd', 'v_visit_gs',\n",
       "       'v_visit_lose', 'v_visit_win', 'h_0_1_gd_rate', 'h_0_1_gs_rate',\n",
       "       'h_0_gd_rate', 'h_0_gs_rate', 'h_1_gd_rate', 'h_1_gs_rate',\n",
       "       'h_2_3_gd_rate', 'h_2_3_gs_rate', 'h_2_gd_rate', 'h_2_gs_rate',\n",
       "       'h_3_gd_rate', 'h_3_gs_rate', 'h_4_gd_rate', 'h_4_gs_rate',\n",
       "       'h_5_gd_rate', 'h_5_gs_rate', 'h_6_gd_rate', 'h_6_gs_rate',\n",
       "       'h_7_gd_rate', 'h_7_gs_rate', 'h_ab_4_gd_rate', 'h_ab_4_gs_rate',\n",
       "       'h_abs_draw_rate', 'h_abs_lose_rate', 'h_abs_win_rate',\n",
       "       'v_0_1_gd_rate', 'v_0_1_gs_rate', 'v_0_gd_rate', 'v_0_gs_rate',\n",
       "       'v_1_gd_rate', 'v_1_gs_rate', 'v_2_3_gd_rate', 'v_2_3_gs_rate',\n",
       "       'v_2_gd_rate', 'v_2_gs_rate', 'v_3_gd_rate', 'v_3_gs_rate',\n",
       "       'v_4_gd_rate', 'v_4_gs_rate', 'v_5_gd_rate', 'v_5_gs_rate',\n",
       "       'v_6_gd_rate', 'v_6_gs_rate', 'v_7_gd_rate', 'v_7_gs_rate',\n",
       "       'v_ab_4_gd_rate', 'v_ab_4_gs_rate', 'v_abs_draw_rate',\n",
       "       'v_abs_lose_rate', 'v_abs_win_rate', 'h_host_draw_rate',\n",
       "       'h_host_g_rate', 'h_host_gd_rate', 'h_host_gs_rate',\n",
       "       'h_host_lose_rate', 'h_host_win_rate', 'v_visit_draw_rate',\n",
       "       'v_visit_g_rate', 'v_visit_gd_rate', 'v_visit_gs_rate',\n",
       "       'v_visit_lose_rate', 'v_visit_win_rate', 'h_avg_abs_gs',\n",
       "       'h_avg_abs_gd', 'v_avg_abs_gs', 'v_avg_abs_gd', 'h_avg_abs_win',\n",
       "       'h_avg_abs_draw', 'h_avg_abs_lose', 'v_avg_abs_win',\n",
       "       'v_avg_abs_draw', 'v_avg_abs_lose', 'avg_init_draw_odd',\n",
       "       'avg_init_lose_odd', 'avg_init_win_odd', 'avg_new_draw_kelly',\n",
       "       'avg_new_draw_odd', 'avg_new_draw_rate', 'avg_new_lose_kelly',\n",
       "       'avg_new_lose_odd', 'avg_new_lose_rate', 'avg_new_win_kelly',\n",
       "       'avg_new_win_odd', 'avg_new_win_rate', 'avg_pay_rate',\n",
       "       'dispersion_draw', 'dispersion_lose', 'dispersion_win', 'id_y',\n",
       "       'max_init_draw_odd', 'max_init_lose_odd', 'max_init_win_odd',\n",
       "       'max_new_draw_kelly', 'max_new_draw_odd', 'max_new_draw_rate',\n",
       "       'max_new_lose_kelly', 'max_new_lose_odd', 'max_new_lose_rate',\n",
       "       'max_new_win_kelly', 'max_new_win_odd', 'max_new_win_rate',\n",
       "       'max_pay_rate', 'min_init_draw_odd', 'min_init_lose_odd',\n",
       "       'min_init_win_odd', 'min_new_draw_kelly', 'min_new_draw_odd',\n",
       "       'min_new_draw_rate', 'min_new_lose_kelly', 'min_new_lose_odd',\n",
       "       'min_new_lose_rate', 'min_new_win_kelly', 'min_new_win_odd',\n",
       "       'min_new_win_rate', 'min_pay_rate', 'std_draw', 'std_lose',\n",
       "       'std_win'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb regressor 尝试预测分差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_df['fix_result'] = train_dataset_df['gs'] - train_dataset_df['gd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.72655\tval-rmse:1.76035\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leewind/.local/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/leewind/.local/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:1.72296\tval-rmse:1.7571\n",
      "[2]\ttrain-rmse:1.71951\tval-rmse:1.75396\n",
      "[3]\ttrain-rmse:1.71604\tval-rmse:1.75112\n",
      "[4]\ttrain-rmse:1.71263\tval-rmse:1.74827\n",
      "[5]\ttrain-rmse:1.70924\tval-rmse:1.74542\n",
      "[6]\ttrain-rmse:1.70608\tval-rmse:1.74233\n",
      "[7]\ttrain-rmse:1.70285\tval-rmse:1.73943\n",
      "[8]\ttrain-rmse:1.69965\tval-rmse:1.73704\n",
      "[9]\ttrain-rmse:1.69645\tval-rmse:1.73413\n",
      "[10]\ttrain-rmse:1.69328\tval-rmse:1.73154\n",
      "[11]\ttrain-rmse:1.69023\tval-rmse:1.72874\n",
      "[12]\ttrain-rmse:1.68726\tval-rmse:1.7263\n",
      "[13]\ttrain-rmse:1.68418\tval-rmse:1.72344\n",
      "[14]\ttrain-rmse:1.68116\tval-rmse:1.7207\n",
      "[15]\ttrain-rmse:1.67829\tval-rmse:1.71816\n",
      "[16]\ttrain-rmse:1.6754\tval-rmse:1.7153\n",
      "[17]\ttrain-rmse:1.67263\tval-rmse:1.7127\n",
      "[18]\ttrain-rmse:1.66991\tval-rmse:1.71066\n",
      "[19]\ttrain-rmse:1.66725\tval-rmse:1.70833\n",
      "[20]\ttrain-rmse:1.66458\tval-rmse:1.70611\n",
      "[21]\ttrain-rmse:1.662\tval-rmse:1.70376\n",
      "[22]\ttrain-rmse:1.6593\tval-rmse:1.70181\n",
      "[23]\ttrain-rmse:1.65678\tval-rmse:1.69975\n",
      "[24]\ttrain-rmse:1.65428\tval-rmse:1.6979\n",
      "[25]\ttrain-rmse:1.65183\tval-rmse:1.6956\n",
      "[26]\ttrain-rmse:1.64945\tval-rmse:1.69381\n",
      "[27]\ttrain-rmse:1.64706\tval-rmse:1.69159\n",
      "[28]\ttrain-rmse:1.64474\tval-rmse:1.68934\n",
      "[29]\ttrain-rmse:1.64239\tval-rmse:1.68736\n",
      "[30]\ttrain-rmse:1.63997\tval-rmse:1.68494\n",
      "[31]\ttrain-rmse:1.63765\tval-rmse:1.68261\n",
      "[32]\ttrain-rmse:1.63541\tval-rmse:1.68109\n",
      "[33]\ttrain-rmse:1.63309\tval-rmse:1.67947\n",
      "[34]\ttrain-rmse:1.63084\tval-rmse:1.67791\n",
      "[35]\ttrain-rmse:1.62862\tval-rmse:1.67652\n",
      "[36]\ttrain-rmse:1.62654\tval-rmse:1.67512\n",
      "[37]\ttrain-rmse:1.6244\tval-rmse:1.67326\n",
      "[38]\ttrain-rmse:1.62234\tval-rmse:1.67148\n",
      "[39]\ttrain-rmse:1.62041\tval-rmse:1.66993\n",
      "[40]\ttrain-rmse:1.61837\tval-rmse:1.66828\n",
      "[41]\ttrain-rmse:1.6164\tval-rmse:1.66656\n",
      "[42]\ttrain-rmse:1.6144\tval-rmse:1.66519\n",
      "[43]\ttrain-rmse:1.61246\tval-rmse:1.66337\n",
      "[44]\ttrain-rmse:1.61051\tval-rmse:1.66194\n",
      "[45]\ttrain-rmse:1.60857\tval-rmse:1.66036\n",
      "[46]\ttrain-rmse:1.60667\tval-rmse:1.65898\n",
      "[47]\ttrain-rmse:1.60476\tval-rmse:1.65769\n",
      "[48]\ttrain-rmse:1.60292\tval-rmse:1.6563\n",
      "[49]\ttrain-rmse:1.60108\tval-rmse:1.65489\n",
      "[50]\ttrain-rmse:1.59922\tval-rmse:1.65344\n",
      "[51]\ttrain-rmse:1.59741\tval-rmse:1.65235\n",
      "[52]\ttrain-rmse:1.59575\tval-rmse:1.65148\n",
      "[53]\ttrain-rmse:1.59394\tval-rmse:1.64989\n",
      "[54]\ttrain-rmse:1.59214\tval-rmse:1.6485\n",
      "[55]\ttrain-rmse:1.59047\tval-rmse:1.6474\n",
      "[56]\ttrain-rmse:1.58887\tval-rmse:1.64603\n",
      "[57]\ttrain-rmse:1.58716\tval-rmse:1.64504\n",
      "[58]\ttrain-rmse:1.58552\tval-rmse:1.64417\n",
      "[59]\ttrain-rmse:1.58396\tval-rmse:1.64299\n",
      "[60]\ttrain-rmse:1.58229\tval-rmse:1.6418\n",
      "[61]\ttrain-rmse:1.58076\tval-rmse:1.64113\n",
      "[62]\ttrain-rmse:1.5793\tval-rmse:1.64022\n",
      "[63]\ttrain-rmse:1.57774\tval-rmse:1.639\n",
      "[64]\ttrain-rmse:1.57623\tval-rmse:1.63808\n",
      "[65]\ttrain-rmse:1.57478\tval-rmse:1.63698\n",
      "[66]\ttrain-rmse:1.5733\tval-rmse:1.63628\n",
      "[67]\ttrain-rmse:1.57198\tval-rmse:1.63529\n",
      "[68]\ttrain-rmse:1.57054\tval-rmse:1.63434\n",
      "[69]\ttrain-rmse:1.56916\tval-rmse:1.63328\n",
      "[70]\ttrain-rmse:1.56787\tval-rmse:1.63264\n",
      "[71]\ttrain-rmse:1.56646\tval-rmse:1.63179\n",
      "[72]\ttrain-rmse:1.56514\tval-rmse:1.63124\n",
      "[73]\ttrain-rmse:1.56375\tval-rmse:1.63021\n",
      "[74]\ttrain-rmse:1.5625\tval-rmse:1.6293\n",
      "[75]\ttrain-rmse:1.56121\tval-rmse:1.62852\n",
      "[76]\ttrain-rmse:1.55983\tval-rmse:1.62792\n",
      "[77]\ttrain-rmse:1.55858\tval-rmse:1.62699\n",
      "[78]\ttrain-rmse:1.5574\tval-rmse:1.62611\n",
      "[79]\ttrain-rmse:1.55615\tval-rmse:1.62527\n",
      "[80]\ttrain-rmse:1.55493\tval-rmse:1.62467\n",
      "[81]\ttrain-rmse:1.55375\tval-rmse:1.62389\n",
      "[82]\ttrain-rmse:1.55258\tval-rmse:1.62341\n",
      "[83]\ttrain-rmse:1.55135\tval-rmse:1.62246\n",
      "[84]\ttrain-rmse:1.55017\tval-rmse:1.6218\n",
      "[85]\ttrain-rmse:1.54903\tval-rmse:1.62105\n",
      "[86]\ttrain-rmse:1.54789\tval-rmse:1.62037\n",
      "[87]\ttrain-rmse:1.54668\tval-rmse:1.61963\n",
      "[88]\ttrain-rmse:1.54553\tval-rmse:1.61908\n",
      "[89]\ttrain-rmse:1.54439\tval-rmse:1.61865\n",
      "[90]\ttrain-rmse:1.54332\tval-rmse:1.61808\n",
      "[91]\ttrain-rmse:1.54223\tval-rmse:1.61742\n",
      "[92]\ttrain-rmse:1.54115\tval-rmse:1.6166\n",
      "[93]\ttrain-rmse:1.54018\tval-rmse:1.61597\n",
      "[94]\ttrain-rmse:1.53901\tval-rmse:1.61539\n",
      "[95]\ttrain-rmse:1.53796\tval-rmse:1.6152\n",
      "[96]\ttrain-rmse:1.537\tval-rmse:1.61464\n",
      "[97]\ttrain-rmse:1.53593\tval-rmse:1.61429\n",
      "[98]\ttrain-rmse:1.53485\tval-rmse:1.61389\n",
      "[99]\ttrain-rmse:1.5338\tval-rmse:1.61376\n",
      "[100]\ttrain-rmse:1.53284\tval-rmse:1.61342\n",
      "[101]\ttrain-rmse:1.53188\tval-rmse:1.61291\n",
      "[102]\ttrain-rmse:1.53099\tval-rmse:1.61237\n",
      "[103]\ttrain-rmse:1.52999\tval-rmse:1.61204\n",
      "[104]\ttrain-rmse:1.52897\tval-rmse:1.61173\n",
      "[105]\ttrain-rmse:1.52805\tval-rmse:1.61114\n",
      "[106]\ttrain-rmse:1.52711\tval-rmse:1.61059\n",
      "[107]\ttrain-rmse:1.52624\tval-rmse:1.61003\n",
      "[108]\ttrain-rmse:1.52533\tval-rmse:1.60936\n",
      "[109]\ttrain-rmse:1.52444\tval-rmse:1.60869\n",
      "[110]\ttrain-rmse:1.52363\tval-rmse:1.60832\n",
      "[111]\ttrain-rmse:1.52283\tval-rmse:1.60818\n",
      "[112]\ttrain-rmse:1.52197\tval-rmse:1.60792\n",
      "[113]\ttrain-rmse:1.52097\tval-rmse:1.60747\n",
      "[114]\ttrain-rmse:1.52019\tval-rmse:1.60693\n",
      "[115]\ttrain-rmse:1.51923\tval-rmse:1.60672\n",
      "[116]\ttrain-rmse:1.51841\tval-rmse:1.60597\n",
      "[117]\ttrain-rmse:1.51755\tval-rmse:1.60604\n",
      "[118]\ttrain-rmse:1.51669\tval-rmse:1.60584\n",
      "[119]\ttrain-rmse:1.51578\tval-rmse:1.60528\n",
      "[120]\ttrain-rmse:1.51501\tval-rmse:1.60476\n",
      "[121]\ttrain-rmse:1.51414\tval-rmse:1.60457\n",
      "[122]\ttrain-rmse:1.51333\tval-rmse:1.60434\n",
      "[123]\ttrain-rmse:1.51249\tval-rmse:1.60405\n",
      "[124]\ttrain-rmse:1.51171\tval-rmse:1.60373\n",
      "[125]\ttrain-rmse:1.51089\tval-rmse:1.60325\n",
      "[126]\ttrain-rmse:1.51005\tval-rmse:1.60309\n",
      "[127]\ttrain-rmse:1.50933\tval-rmse:1.60278\n",
      "[128]\ttrain-rmse:1.50864\tval-rmse:1.60239\n",
      "[129]\ttrain-rmse:1.50783\tval-rmse:1.60228\n",
      "[130]\ttrain-rmse:1.5071\tval-rmse:1.60185\n",
      "[131]\ttrain-rmse:1.50643\tval-rmse:1.60174\n",
      "[132]\ttrain-rmse:1.50555\tval-rmse:1.60173\n",
      "[133]\ttrain-rmse:1.50484\tval-rmse:1.60157\n",
      "[134]\ttrain-rmse:1.50425\tval-rmse:1.60126\n",
      "[135]\ttrain-rmse:1.5035\tval-rmse:1.60088\n",
      "[136]\ttrain-rmse:1.50256\tval-rmse:1.60055\n",
      "[137]\ttrain-rmse:1.50185\tval-rmse:1.60038\n",
      "[138]\ttrain-rmse:1.50105\tval-rmse:1.60008\n",
      "[139]\ttrain-rmse:1.50026\tval-rmse:1.59985\n",
      "[140]\ttrain-rmse:1.49958\tval-rmse:1.59948\n",
      "[141]\ttrain-rmse:1.4989\tval-rmse:1.59924\n",
      "[142]\ttrain-rmse:1.49817\tval-rmse:1.59898\n",
      "[143]\ttrain-rmse:1.49746\tval-rmse:1.59889\n",
      "[144]\ttrain-rmse:1.49672\tval-rmse:1.59829\n",
      "[145]\ttrain-rmse:1.49603\tval-rmse:1.59825\n",
      "[146]\ttrain-rmse:1.49522\tval-rmse:1.59796\n",
      "[147]\ttrain-rmse:1.49462\tval-rmse:1.59776\n",
      "[148]\ttrain-rmse:1.49399\tval-rmse:1.59764\n",
      "[149]\ttrain-rmse:1.49334\tval-rmse:1.59763\n",
      "[150]\ttrain-rmse:1.49268\tval-rmse:1.59769\n",
      "[151]\ttrain-rmse:1.49195\tval-rmse:1.59749\n",
      "[152]\ttrain-rmse:1.49136\tval-rmse:1.59718\n",
      "[153]\ttrain-rmse:1.49071\tval-rmse:1.59694\n",
      "[154]\ttrain-rmse:1.49011\tval-rmse:1.59682\n",
      "[155]\ttrain-rmse:1.48947\tval-rmse:1.59647\n",
      "[156]\ttrain-rmse:1.4888\tval-rmse:1.59634\n",
      "[157]\ttrain-rmse:1.48829\tval-rmse:1.59634\n",
      "[158]\ttrain-rmse:1.48757\tval-rmse:1.59642\n",
      "[159]\ttrain-rmse:1.48691\tval-rmse:1.59623\n",
      "[160]\ttrain-rmse:1.48629\tval-rmse:1.59619\n",
      "[161]\ttrain-rmse:1.48564\tval-rmse:1.59609\n",
      "[162]\ttrain-rmse:1.48515\tval-rmse:1.59591\n",
      "[163]\ttrain-rmse:1.48467\tval-rmse:1.59583\n",
      "[164]\ttrain-rmse:1.48402\tval-rmse:1.59575\n",
      "[165]\ttrain-rmse:1.4834\tval-rmse:1.5958\n",
      "[166]\ttrain-rmse:1.48278\tval-rmse:1.59554\n",
      "[167]\ttrain-rmse:1.48211\tval-rmse:1.59532\n",
      "[168]\ttrain-rmse:1.48161\tval-rmse:1.59529\n",
      "[169]\ttrain-rmse:1.481\tval-rmse:1.59524\n",
      "[170]\ttrain-rmse:1.48046\tval-rmse:1.59496\n",
      "[171]\ttrain-rmse:1.47985\tval-rmse:1.59484\n",
      "[172]\ttrain-rmse:1.47931\tval-rmse:1.59445\n",
      "[173]\ttrain-rmse:1.47873\tval-rmse:1.59442\n",
      "[174]\ttrain-rmse:1.47819\tval-rmse:1.59437\n",
      "[175]\ttrain-rmse:1.47759\tval-rmse:1.59432\n",
      "[176]\ttrain-rmse:1.47696\tval-rmse:1.59421\n",
      "[177]\ttrain-rmse:1.47646\tval-rmse:1.59398\n",
      "[178]\ttrain-rmse:1.47583\tval-rmse:1.59368\n",
      "[179]\ttrain-rmse:1.47532\tval-rmse:1.59374\n",
      "[180]\ttrain-rmse:1.47472\tval-rmse:1.5934\n",
      "[181]\ttrain-rmse:1.47416\tval-rmse:1.59324\n",
      "[182]\ttrain-rmse:1.47358\tval-rmse:1.59291\n",
      "[183]\ttrain-rmse:1.47308\tval-rmse:1.59276\n",
      "[184]\ttrain-rmse:1.47266\tval-rmse:1.59278\n",
      "[185]\ttrain-rmse:1.47213\tval-rmse:1.5928\n",
      "[186]\ttrain-rmse:1.47149\tval-rmse:1.59289\n",
      "[187]\ttrain-rmse:1.47094\tval-rmse:1.59262\n",
      "[188]\ttrain-rmse:1.47046\tval-rmse:1.59245\n",
      "[189]\ttrain-rmse:1.46991\tval-rmse:1.59251\n",
      "[190]\ttrain-rmse:1.46951\tval-rmse:1.5927\n",
      "[191]\ttrain-rmse:1.469\tval-rmse:1.59251\n",
      "[192]\ttrain-rmse:1.46859\tval-rmse:1.59239\n",
      "[193]\ttrain-rmse:1.46813\tval-rmse:1.59236\n",
      "[194]\ttrain-rmse:1.46772\tval-rmse:1.5922\n",
      "[195]\ttrain-rmse:1.4671\tval-rmse:1.5922\n",
      "[196]\ttrain-rmse:1.46639\tval-rmse:1.59207\n",
      "[197]\ttrain-rmse:1.46585\tval-rmse:1.59213\n",
      "[198]\ttrain-rmse:1.46538\tval-rmse:1.59211\n",
      "[199]\ttrain-rmse:1.46499\tval-rmse:1.59211\n",
      "[200]\ttrain-rmse:1.4644\tval-rmse:1.59187\n",
      "[201]\ttrain-rmse:1.46383\tval-rmse:1.59198\n",
      "[202]\ttrain-rmse:1.46341\tval-rmse:1.59208\n",
      "[203]\ttrain-rmse:1.46299\tval-rmse:1.59209\n",
      "[204]\ttrain-rmse:1.46251\tval-rmse:1.59218\n",
      "[205]\ttrain-rmse:1.46207\tval-rmse:1.59215\n",
      "[206]\ttrain-rmse:1.46161\tval-rmse:1.59203\n",
      "[207]\ttrain-rmse:1.4612\tval-rmse:1.59196\n",
      "[208]\ttrain-rmse:1.46065\tval-rmse:1.59181\n",
      "[209]\ttrain-rmse:1.46011\tval-rmse:1.59177\n",
      "[210]\ttrain-rmse:1.45962\tval-rmse:1.59176\n",
      "[211]\ttrain-rmse:1.45914\tval-rmse:1.59181\n",
      "[212]\ttrain-rmse:1.45872\tval-rmse:1.59186\n",
      "[213]\ttrain-rmse:1.45821\tval-rmse:1.59185\n",
      "[214]\ttrain-rmse:1.45784\tval-rmse:1.59199\n",
      "[215]\ttrain-rmse:1.45748\tval-rmse:1.59195\n",
      "[216]\ttrain-rmse:1.45699\tval-rmse:1.59191\n",
      "[217]\ttrain-rmse:1.45662\tval-rmse:1.59189\n",
      "[218]\ttrain-rmse:1.45605\tval-rmse:1.5919\n",
      "[219]\ttrain-rmse:1.45552\tval-rmse:1.59209\n",
      "[220]\ttrain-rmse:1.45509\tval-rmse:1.59177\n",
      "[221]\ttrain-rmse:1.45472\tval-rmse:1.59182\n",
      "[222]\ttrain-rmse:1.45432\tval-rmse:1.59187\n",
      "[223]\ttrain-rmse:1.45388\tval-rmse:1.59175\n",
      "[224]\ttrain-rmse:1.45331\tval-rmse:1.59154\n",
      "[225]\ttrain-rmse:1.45287\tval-rmse:1.59161\n",
      "[226]\ttrain-rmse:1.45238\tval-rmse:1.59162\n",
      "[227]\ttrain-rmse:1.45193\tval-rmse:1.59159\n",
      "[228]\ttrain-rmse:1.45159\tval-rmse:1.59147\n",
      "[229]\ttrain-rmse:1.45123\tval-rmse:1.59149\n",
      "[230]\ttrain-rmse:1.45061\tval-rmse:1.59155\n",
      "[231]\ttrain-rmse:1.45021\tval-rmse:1.59146\n",
      "[232]\ttrain-rmse:1.44972\tval-rmse:1.59169\n",
      "[233]\ttrain-rmse:1.44941\tval-rmse:1.59179\n",
      "[234]\ttrain-rmse:1.44903\tval-rmse:1.59194\n",
      "[235]\ttrain-rmse:1.4486\tval-rmse:1.59204\n",
      "[236]\ttrain-rmse:1.44811\tval-rmse:1.59191\n",
      "[237]\ttrain-rmse:1.44761\tval-rmse:1.59187\n",
      "[238]\ttrain-rmse:1.44714\tval-rmse:1.59182\n",
      "[239]\ttrain-rmse:1.44667\tval-rmse:1.59184\n",
      "[240]\ttrain-rmse:1.44614\tval-rmse:1.59192\n",
      "[241]\ttrain-rmse:1.44568\tval-rmse:1.59206\n",
      "[242]\ttrain-rmse:1.44518\tval-rmse:1.59212\n",
      "[243]\ttrain-rmse:1.4448\tval-rmse:1.59211\n",
      "[244]\ttrain-rmse:1.44437\tval-rmse:1.59224\n",
      "[245]\ttrain-rmse:1.44392\tval-rmse:1.59223\n",
      "[246]\ttrain-rmse:1.4436\tval-rmse:1.59239\n",
      "[247]\ttrain-rmse:1.4433\tval-rmse:1.59233\n",
      "[248]\ttrain-rmse:1.44285\tval-rmse:1.59226\n",
      "[249]\ttrain-rmse:1.44247\tval-rmse:1.59235\n",
      "[250]\ttrain-rmse:1.44202\tval-rmse:1.5925\n",
      "[251]\ttrain-rmse:1.44168\tval-rmse:1.59253\n",
      "[252]\ttrain-rmse:1.44121\tval-rmse:1.5925\n",
      "[253]\ttrain-rmse:1.4408\tval-rmse:1.59257\n",
      "[254]\ttrain-rmse:1.44048\tval-rmse:1.59268\n",
      "[255]\ttrain-rmse:1.44024\tval-rmse:1.59273\n",
      "[256]\ttrain-rmse:1.4398\tval-rmse:1.59282\n",
      "[257]\ttrain-rmse:1.43931\tval-rmse:1.59244\n",
      "[258]\ttrain-rmse:1.43879\tval-rmse:1.5923\n",
      "[259]\ttrain-rmse:1.4383\tval-rmse:1.59216\n",
      "[260]\ttrain-rmse:1.43795\tval-rmse:1.59222\n",
      "[261]\ttrain-rmse:1.43763\tval-rmse:1.59217\n",
      "[262]\ttrain-rmse:1.43709\tval-rmse:1.59231\n",
      "[263]\ttrain-rmse:1.4366\tval-rmse:1.59232\n",
      "[264]\ttrain-rmse:1.43623\tval-rmse:1.59248\n",
      "[265]\ttrain-rmse:1.43598\tval-rmse:1.59247\n",
      "[266]\ttrain-rmse:1.43556\tval-rmse:1.59242\n",
      "[267]\ttrain-rmse:1.4352\tval-rmse:1.59231\n",
      "[268]\ttrain-rmse:1.43491\tval-rmse:1.59225\n",
      "[269]\ttrain-rmse:1.43452\tval-rmse:1.59213\n",
      "[270]\ttrain-rmse:1.43417\tval-rmse:1.5921\n",
      "[271]\ttrain-rmse:1.43394\tval-rmse:1.59207\n",
      "[272]\ttrain-rmse:1.43366\tval-rmse:1.59206\n",
      "[273]\ttrain-rmse:1.4332\tval-rmse:1.59218\n",
      "[274]\ttrain-rmse:1.43282\tval-rmse:1.59218\n",
      "[275]\ttrain-rmse:1.43239\tval-rmse:1.59231\n",
      "[276]\ttrain-rmse:1.43206\tval-rmse:1.59234\n",
      "[277]\ttrain-rmse:1.43173\tval-rmse:1.59237\n",
      "[278]\ttrain-rmse:1.43142\tval-rmse:1.59234\n",
      "[279]\ttrain-rmse:1.43093\tval-rmse:1.59233\n",
      "[280]\ttrain-rmse:1.43063\tval-rmse:1.59238\n",
      "[281]\ttrain-rmse:1.43037\tval-rmse:1.59252\n",
      "[282]\ttrain-rmse:1.42988\tval-rmse:1.59259\n",
      "[283]\ttrain-rmse:1.42952\tval-rmse:1.59266\n",
      "[284]\ttrain-rmse:1.4292\tval-rmse:1.59271\n",
      "[285]\ttrain-rmse:1.42881\tval-rmse:1.59281\n",
      "[286]\ttrain-rmse:1.42845\tval-rmse:1.59276\n",
      "[287]\ttrain-rmse:1.42797\tval-rmse:1.59287\n",
      "[288]\ttrain-rmse:1.42769\tval-rmse:1.59278\n",
      "[289]\ttrain-rmse:1.42729\tval-rmse:1.59281\n",
      "[290]\ttrain-rmse:1.427\tval-rmse:1.59282\n",
      "[291]\ttrain-rmse:1.42664\tval-rmse:1.59271\n",
      "[292]\ttrain-rmse:1.4263\tval-rmse:1.59283\n",
      "[293]\ttrain-rmse:1.42605\tval-rmse:1.59284\n",
      "[294]\ttrain-rmse:1.42575\tval-rmse:1.59298\n",
      "[295]\ttrain-rmse:1.42551\tval-rmse:1.59292\n",
      "[296]\ttrain-rmse:1.42509\tval-rmse:1.59312\n",
      "[297]\ttrain-rmse:1.42471\tval-rmse:1.59306\n",
      "[298]\ttrain-rmse:1.42438\tval-rmse:1.59299\n",
      "[299]\ttrain-rmse:1.42409\tval-rmse:1.59306\n",
      "[300]\ttrain-rmse:1.42373\tval-rmse:1.59315\n",
      "[301]\ttrain-rmse:1.42324\tval-rmse:1.59311\n",
      "[302]\ttrain-rmse:1.42282\tval-rmse:1.59298\n",
      "[303]\ttrain-rmse:1.42245\tval-rmse:1.59301\n",
      "[304]\ttrain-rmse:1.42201\tval-rmse:1.59316\n",
      "[305]\ttrain-rmse:1.42173\tval-rmse:1.593\n",
      "[306]\ttrain-rmse:1.42117\tval-rmse:1.59282\n",
      "[307]\ttrain-rmse:1.42072\tval-rmse:1.59284\n",
      "[308]\ttrain-rmse:1.42038\tval-rmse:1.59282\n",
      "[309]\ttrain-rmse:1.41994\tval-rmse:1.59277\n",
      "[310]\ttrain-rmse:1.41948\tval-rmse:1.59271\n",
      "[311]\ttrain-rmse:1.41899\tval-rmse:1.59279\n",
      "[312]\ttrain-rmse:1.41867\tval-rmse:1.59289\n",
      "[313]\ttrain-rmse:1.41843\tval-rmse:1.59292\n",
      "[314]\ttrain-rmse:1.41819\tval-rmse:1.59293\n",
      "[315]\ttrain-rmse:1.41792\tval-rmse:1.59319\n",
      "[316]\ttrain-rmse:1.41749\tval-rmse:1.5932\n",
      "[317]\ttrain-rmse:1.41715\tval-rmse:1.5931\n",
      "[318]\ttrain-rmse:1.41684\tval-rmse:1.59321\n",
      "[319]\ttrain-rmse:1.41635\tval-rmse:1.59333\n",
      "[320]\ttrain-rmse:1.41606\tval-rmse:1.59343\n",
      "[321]\ttrain-rmse:1.41571\tval-rmse:1.59341\n",
      "[322]\ttrain-rmse:1.41544\tval-rmse:1.59349\n",
      "[323]\ttrain-rmse:1.41516\tval-rmse:1.59353\n",
      "[324]\ttrain-rmse:1.4148\tval-rmse:1.59375\n",
      "[325]\ttrain-rmse:1.41446\tval-rmse:1.59373\n",
      "[326]\ttrain-rmse:1.41426\tval-rmse:1.59385\n",
      "[327]\ttrain-rmse:1.41395\tval-rmse:1.59403\n",
      "[328]\ttrain-rmse:1.41379\tval-rmse:1.594\n",
      "[329]\ttrain-rmse:1.41359\tval-rmse:1.59395\n",
      "[330]\ttrain-rmse:1.41299\tval-rmse:1.59415\n",
      "[331]\ttrain-rmse:1.41268\tval-rmse:1.59411\n",
      "Stopping. Best iteration:\n",
      "[231]\ttrain-rmse:1.45021\tval-rmse:1.59146\n",
      "\n",
      "231\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "x_columns = [\n",
    "#     'win_bet_return', 'draw_bet_return', 'lose_bet_return', \n",
    "    'year', 'month',\n",
    "#     'home_team_encoder', 'visit_team_encoder', 'game_encoder',\n",
    "    \n",
    "    'h_avg_abs_gs',\n",
    "    'h_avg_abs_gd', \n",
    "    'v_avg_abs_gs', \n",
    "    'v_avg_abs_gd', \n",
    "    'h_avg_abs_win',\n",
    "    'h_avg_abs_draw', 'h_avg_abs_lose', 'v_avg_abs_win',\n",
    "    'v_avg_abs_draw', 'v_avg_abs_lose',\n",
    "    \n",
    "    'h_0_1_gd', \n",
    "    'h_0_1_gs', \n",
    "    'h_0_gd', \n",
    "    'h_0_gs', \n",
    "    'h_1_gd', \n",
    "    'h_1_gs',\n",
    "    'h_2_3_gd', \n",
    "    'h_2_3_gs', \n",
    "    'h_2_gd', 'h_2_gs', 'h_3_gd', 'h_3_gs',\n",
    "    'h_4_gd', 'h_4_gs', 'h_5_gd', 'h_5_gs', 'h_6_gd', 'h_6_gs',\n",
    "    'h_7_gd', 'h_7_gs', 'h_ab_4_gd', 'h_ab_4_gs', 'h_abs_avg_g',\n",
    "    'h_abs_avg_gd', 'h_abs_avg_gs', 'h_abs_draw', 'h_abs_g',\n",
    "    'h_abs_gd', 'h_abs_gs', 'h_abs_lose', 'h_abs_win', 'h_count',\n",
    "    'h_host_count', \n",
    "    'h_host_draw', \n",
    "    'h_host_g', 'h_host_gd',\n",
    "    'h_host_gs', \n",
    "    'h_host_lose', 'h_host_win', \n",
    "    \n",
    "    'v_0_1_gd',\n",
    "    'v_0_1_gs', 'v_0_gd', 'v_0_gs', 'v_1_gd', 'v_1_gs', 'v_2_3_gd',\n",
    "    'v_2_3_gs', 'v_2_gd', 'v_2_gs', 'v_3_gd', 'v_3_gs', 'v_4_gd',\n",
    "    'v_4_gs', 'v_5_gd', 'v_5_gs', 'v_6_gd', 'v_6_gs', 'v_7_gd',\n",
    "    'v_7_gs', 'v_ab_4_gd', 'v_ab_4_gs', 'v_abs_avg_g', 'v_abs_avg_gd',\n",
    "    'v_abs_avg_gs', 'v_abs_draw', 'v_abs_g', 'v_abs_gd', 'v_abs_gs',\n",
    "    'v_abs_lose', 'v_abs_win', \n",
    "    \n",
    "    'v_count', \n",
    "    'v_visit_count',\n",
    "    'v_visit_draw', \n",
    "    'v_visit_g', 'v_visit_gd', 'v_visit_gs',\n",
    "    'v_visit_lose', 'v_visit_win',\n",
    "    \n",
    "    'h_0_1_gd_rate', 'h_0_1_gs_rate',\n",
    "    'h_0_gd_rate', 'h_0_gs_rate', 'h_1_gd_rate', 'h_1_gs_rate',\n",
    "    'h_2_3_gd_rate', 'h_2_3_gs_rate', 'h_2_gd_rate', 'h_2_gs_rate',\n",
    "    'h_3_gd_rate', 'h_3_gs_rate', 'h_4_gd_rate', 'h_4_gs_rate',\n",
    "    'h_5_gd_rate', 'h_5_gs_rate', 'h_6_gd_rate', 'h_6_gs_rate',\n",
    "    'h_7_gd_rate', 'h_7_gs_rate', 'h_ab_4_gd_rate', 'h_ab_4_gs_rate',\n",
    "    'h_abs_draw_rate', 'h_abs_lose_rate', 'h_abs_win_rate',\n",
    "    'v_0_1_gd_rate', 'v_0_1_gs_rate', 'v_0_gd_rate', 'v_0_gs_rate',\n",
    "    'v_1_gd_rate', 'v_1_gs_rate', 'v_2_3_gd_rate', 'v_2_3_gs_rate',\n",
    "    'v_2_gd_rate', 'v_2_gs_rate', 'v_3_gd_rate', 'v_3_gs_rate',\n",
    "    'v_4_gd_rate', 'v_4_gs_rate', 'v_5_gd_rate', 'v_5_gs_rate',\n",
    "    'v_6_gd_rate', 'v_6_gs_rate', 'v_7_gd_rate', 'v_7_gs_rate',\n",
    "    \n",
    "    'v_ab_4_gd_rate', \n",
    "    'v_ab_4_gs_rate', \n",
    "    'v_abs_draw_rate',\n",
    "    'v_abs_lose_rate', \n",
    "    'v_abs_win_rate',\n",
    "    'h_host_draw_rate',\n",
    "    'h_host_g_rate', \n",
    "    'h_host_gd_rate', \n",
    "    'h_host_gs_rate',\n",
    "    'h_host_lose_rate', \n",
    "    'h_host_win_rate', \n",
    "    'v_visit_draw_rate',\n",
    "    'v_visit_g_rate', \n",
    "    'v_visit_gd_rate', \n",
    "    'v_visit_gs_rate',\n",
    "    'v_visit_lose_rate', \n",
    "    'v_visit_win_rate',\n",
    "    \n",
    "    'avg_init_draw_odd',\n",
    "    'avg_init_lose_odd', 'avg_init_win_odd', 'avg_new_draw_kelly',\n",
    "    'avg_new_draw_odd', 'avg_new_draw_rate', 'avg_new_lose_kelly',\n",
    "    'avg_new_lose_odd', 'avg_new_lose_rate', 'avg_new_win_kelly',\n",
    "    'avg_new_win_odd', 'avg_new_win_rate', 'avg_pay_rate',\n",
    "    'dispersion_draw', 'dispersion_lose', 'dispersion_win', \n",
    "    'max_init_draw_odd', 'max_init_lose_odd', 'max_init_win_odd',\n",
    "    'max_new_draw_kelly', 'max_new_draw_odd', 'max_new_draw_rate',\n",
    "    'max_new_lose_kelly', 'max_new_lose_odd', 'max_new_lose_rate',\n",
    "    'max_new_win_kelly', 'max_new_win_odd', 'max_new_win_rate',\n",
    "    'max_pay_rate', 'min_init_draw_odd', 'min_init_lose_odd',\n",
    "    'min_init_win_odd', 'min_new_draw_kelly', 'min_new_draw_odd',\n",
    "    'min_new_draw_rate', 'min_new_lose_kelly', 'min_new_lose_odd',\n",
    "    'min_new_lose_rate', 'min_new_win_kelly', 'min_new_win_odd',\n",
    "    'min_new_win_rate', 'min_pay_rate', 'std_draw', 'std_lose',\n",
    "    'std_win'\n",
    "]\n",
    "    \n",
    "params={\n",
    "    'booster':'gbtree',\n",
    "    # 这里手写数字是0-9，是一个多类的问题，因此采用了multisoft多分类器，\n",
    "    'objective': 'reg:linear', \n",
    "#     'objective': 'multi:softprob',\n",
    "#     'num_class':3, # 类数，与 multisoftmax 并用\n",
    "    \n",
    "    'gamma':0.01,  # 在树的叶子节点下一个分区的最小损失，越大算法模型越保守 。[0:]\n",
    "    'max_depth':8, # 构建树的深度 [1:]\n",
    "    \n",
    "    #'lambda':450,  # L2 正则项权重\n",
    "    'subsample':0.7, # 采样训练数据，设置为0.5，随机选择一般的数据实例 (0:1]\n",
    "    'colsample_bytree':0.7, # 构建树树时的采样比率 (0:1]\n",
    "    #'min_child_weight':12, # 节点的最少特征数\n",
    "    'silent':1 ,\n",
    "    \n",
    "#     这部分需要调整\n",
    "#     'eta': 0.05, # 如同学习率\n",
    "    'eta': 0.01, # 如同学习率\n",
    "    \n",
    "    \n",
    "    'seed':2018,\n",
    "    'nthread':4,# cpu 线程数,根据自己U的个数适当调整\n",
    "}\n",
    "\n",
    "t = train_dataset_df\n",
    "\n",
    "train_dataset = t[t['year'] < 2019]\n",
    "test_dataset = t[t['year'] == 2019]\n",
    "\n",
    "valid_dataset = test_dataset[test_dataset['month'] < 3]\n",
    "test_dataset = test_dataset[test_dataset['month'] >= 3]\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_dataset[x_columns], label=train_dataset['fix_result'])\n",
    "xgtest = xgb.DMatrix(test_dataset[x_columns], label=test_dataset['fix_result'])\n",
    "xgvalid = xgb.DMatrix(valid_dataset[x_columns], label=valid_dataset['fix_result'])\n",
    "\n",
    "watchlist = [(xgtrain, 'train'),(xgvalid, 'val')]\n",
    "\n",
    "num_rounds = 10000\n",
    "stop_rounds = 100\n",
    "\n",
    "# num_rounds = 10000\n",
    "# stop_rounds = 300\n",
    "\n",
    "model = xgb.train(params, xgtrain, num_rounds, watchlist, early_stopping_rounds=stop_rounds)\n",
    "print(model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19064993,  0.08287174,  0.9153619 , ...,  0.9269872 ,\n",
       "        0.8028513 , -0.55736434], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['preds'] = preds\n",
    "# test_dataset[['gs', 'gd', 'fix_result', 'preds']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_score(row):\n",
    "    if row.preds > 0:\n",
    "        return -(math.floor(row.preds)) - 0.25\n",
    "#         return -(math.floor(row.preds)) + 0.5\n",
    "    else:\n",
    "        return -(math.ceil(row.preds)) + 0.25\n",
    "#         return -(math.ceil(row.preds)) - 0.5\n",
    "\n",
    "# test_dataset['rq'] = test_dataset.apply(lambda row: math.floor(math.floor(row.preds * 10) / 5) * 0.5, axis=1)\n",
    "# test_dataset['rq'] = test_dataset.apply(lambda row: get_score(row), axis=1)\n",
    "\n",
    "\n",
    "test_dataset['rq'] = -test_dataset['preds']\n",
    "test_dataset['rq_r'] = test_dataset['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['rq_result'] = test_dataset['rq'] + test_dataset['fix_result']\n",
    "test_dataset['pred_rq_result'] = test_dataset.apply(lambda row: 1 if row.rq_result > 0 else 0, axis=1)\n",
    "\n",
    "# a = test_dataset[test_dataset['rq'] >= 0]\n",
    "# a = test_dataset\n",
    "# print(len(a[a['pred_rq_result'] == 1])/ len(a))\n",
    "# a[['matchid', 'gs', 'gd', 'fix_result', 'rq', 'rq_result', 'pred_rq_result']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997, 1131)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.merge(test_dataset, train_game_offset_df, on='matchid', how='left')\n",
    "test_df = test_df.dropna()\n",
    "len(test_df), len(test_dataset)\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['offset_result'] = test_df.apply(lambda row: 1 if (row.fix_result + row.new_offset_val) > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pred_offset_result(row):\n",
    "#     if row.rq < 0 and row.new_offset_val < 0:\n",
    "#         if (row.new_offset_val - row.rq) > 0:\n",
    "#             return 0\n",
    "#         else:\n",
    "#             return 1\n",
    "#     else:\n",
    "    if (row.new_offset_val - row.rq) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# test_df['pred_offset_result'] = test_df.apply(lambda row: 1 if (row.new_offset_val - row.rq) > 0 else 0, axis=1)\n",
    "test_df['pred_offset_result'] = test_df.apply(lambda row: cal_pred_offset_result(row), axis=1)\n",
    "\n",
    "# test_df['pred_offset_result'] = test_df.apply(lambda row: 1 if (row.fix_result + row.rq) > 0 else 0, axis=1)\n",
    "test_df['gap'] = test_df['new_offset_val'] - test_df['rq']\n",
    "# test_df['pred_offset_result'] = test_df.apply(lambda row: 1 if (row.offset_result - row.rq) > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['check'] = test_df.apply(lambda row: 1 if row.offset_result == row.pred_offset_result else 0, axis=1)\n",
    "# len(test_df[(test_df['check'] == 1) & (test_df['rq'] < 0)]) / len(test_df[test_df['rq'] < 0])\n",
    "\n",
    "b = test_df[abs(test_df['gap'])> 0.4]\n",
    "len(test_df[(test_df['check'] == 1)  & (abs(test_df['gap'])> 0.4) ])/len(test_df[abs(test_df['gap'])> 0.4])\n",
    "# b = test_df[test_df['pred_offset_result'] == 0]\n",
    "# len(b[b['check']==1])/len(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchid</th>\n",
       "      <th>gs</th>\n",
       "      <th>gd</th>\n",
       "      <th>fix_result</th>\n",
       "      <th>rq</th>\n",
       "      <th>new_offset_val</th>\n",
       "      <th>offset_result</th>\n",
       "      <th>pred_offset_result</th>\n",
       "      <th>check</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2405431</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1.337834</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.912166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2405428</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.781775</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.718225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2406876</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.667379</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.417379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2405479</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.338514</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.411486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>2508506</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-0.485837</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.485837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2514329</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.059707</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.440293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2436055</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.401250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2428903</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.114645</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.635355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2411825</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1.152105</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.402105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2406712</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.495294</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.745294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>2508537</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>-0.684775</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.434775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>2437360</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.833288</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.583288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>2436063</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.698930</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.551070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>2508525</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.922860</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>2508560</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1.196559</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.446559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>2508561</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.924602</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.424602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>2436078</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.819185</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.430815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>2405520</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.223960</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.473960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>2508530</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.776911</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.526911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>2514446</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.002987</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.497013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>2502910</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.157362</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.407362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>2405553</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.504406</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.495594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>2428953</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.910363</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.589637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>2406459</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.663574</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.413574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>2508637</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.578756</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.421244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>2514512</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.061809</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.561809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>2406496</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.753928</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.503928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>2437458</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.557071</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.442929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>2502928</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.535693</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.464307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>2415165</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.707533</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.542467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>2405589</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.658681</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>2406473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.340897</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.659103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>2405601</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.704475</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.454475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>2415173</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.039512</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.460488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>2406522</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.927563</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>2411848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.704048</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.454048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>2437470</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.044129</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.455871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>2413991</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.848837</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.901163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2428971</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.983111</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.483111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2436120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>2413996</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.037195</td>\n",
       "      <td>-3.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.712805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>2502967</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.076145</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.423855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>2429039</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.285769</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.464231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>2508736</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-0.746532</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.496532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>2502974</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.902821</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.402821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      matchid  gs  gd  fix_result        rq  new_offset_val  offset_result  \\\n",
       "59    2405431   0   2          -2  1.337834            2.25              1   \n",
       "94    2405428   4   2           2 -1.781775           -2.50              0   \n",
       "118   2406876   1   1           0 -0.667379           -0.25              0   \n",
       "163   2405479   2   0           2 -2.338514           -2.75              0   \n",
       "165   2508506   0   2          -2 -0.485837            0.00              0   \n",
       "172   2514329   1   3          -2  0.059707            0.50              0   \n",
       "200   2436055   1   4          -3  0.598750            1.00              0   \n",
       "202   2428903   1   2          -1  1.114645            1.75              1   \n",
       "223   2411825   0   2          -2  1.152105            0.75              0   \n",
       "227   2406712   1   0           1 -0.495294            0.25              1   \n",
       "279   2508537   1   3          -2 -0.684775           -0.25              0   \n",
       "293   2437360   3   1           2 -0.833288           -0.25              1   \n",
       "319   2436063   1   4          -3  0.698930            1.25              0   \n",
       "337   2508525   2   1           1 -1.922860           -1.50              0   \n",
       "345   2508560   0   2          -2 -1.196559           -0.75              0   \n",
       "366   2508561   4   0           4 -0.924602           -0.50              1   \n",
       "399   2436078   2   0           2 -1.819185           -2.25              0   \n",
       "406   2405520   3   2           1 -1.223960           -0.75              1   \n",
       "408   2508530   2   1           1 -0.776911           -0.25              1   \n",
       "506   2514446   2   0           2 -1.002987           -1.50              1   \n",
       "576   2502910   3   2           1 -1.157362           -0.75              1   \n",
       "582   2405553   3   3           0  0.504406            1.00              1   \n",
       "592   2428953   1   4          -3  0.910363            1.50              0   \n",
       "624   2406459   3   1           2 -0.663574           -0.25              1   \n",
       "678   2508637   2   1           1 -0.578756           -1.00              0   \n",
       "686   2514512   1   0           1 -1.061809           -0.50              1   \n",
       "756   2406496   1   1           0 -0.753928           -0.25              0   \n",
       "772   2437458   3   1           2  0.557071            1.00              1   \n",
       "787   2502928   0   1          -1  0.535693            1.00              0   \n",
       "816   2415165   3   0           3  0.707533            1.25              1   \n",
       "821   2405589   2   1           1 -0.658681           -0.25              1   \n",
       "836   2406473   0   0           0 -1.340897           -2.00              0   \n",
       "893   2405601   3   1           2 -0.704475           -0.25              1   \n",
       "902   2415173   1   2          -1 -1.039512           -1.50              0   \n",
       "915   2406522   1   1           0 -0.927563           -0.50              0   \n",
       "959   2411848   0   0           0 -0.704048           -0.25              0   \n",
       "971   2437470   5   0           5 -1.044129           -1.50              1   \n",
       "989   2413991   1   1           0  1.848837            2.75              1   \n",
       "996   2428971   1   4          -3  0.983111            0.50              0   \n",
       "999   2436120   1   0           1  0.759853            1.25              1   \n",
       "1061  2413996   3   1           2 -3.037195           -3.75              0   \n",
       "1063  2502967   2   4          -2  0.076145            0.50              0   \n",
       "1070  2429039   5   1           4 -2.285769           -2.75              1   \n",
       "1074  2508736   0   2          -2 -0.746532           -0.25              0   \n",
       "1116  2502974   2   3          -1  0.902821            0.50              0   \n",
       "\n",
       "      pred_offset_result  check       gap  \n",
       "59                     1      1  0.912166  \n",
       "94                     0      1 -0.718225  \n",
       "118                    1      0  0.417379  \n",
       "163                    0      1 -0.411486  \n",
       "165                    1      0  0.485837  \n",
       "172                    1      0  0.440293  \n",
       "200                    1      0  0.401250  \n",
       "202                    1      1  0.635355  \n",
       "223                    0      1 -0.402105  \n",
       "227                    1      1  0.745294  \n",
       "279                    1      0  0.434775  \n",
       "293                    1      1  0.583288  \n",
       "319                    1      0  0.551070  \n",
       "337                    1      0  0.422860  \n",
       "345                    1      0  0.446559  \n",
       "366                    1      1  0.424602  \n",
       "399                    0      1 -0.430815  \n",
       "406                    1      1  0.473960  \n",
       "408                    1      1  0.526911  \n",
       "506                    0      0 -0.497013  \n",
       "576                    1      1  0.407362  \n",
       "582                    1      1  0.495594  \n",
       "592                    1      0  0.589637  \n",
       "624                    1      1  0.413574  \n",
       "678                    0      1 -0.421244  \n",
       "686                    1      1  0.561809  \n",
       "756                    1      0  0.503928  \n",
       "772                    1      1  0.442929  \n",
       "787                    1      0  0.464307  \n",
       "816                    1      1  0.542467  \n",
       "821                    1      1  0.408681  \n",
       "836                    0      1 -0.659103  \n",
       "893                    1      1  0.454475  \n",
       "902                    0      1 -0.460488  \n",
       "915                    1      0  0.427563  \n",
       "959                    1      0  0.454048  \n",
       "971                    0      0 -0.455871  \n",
       "989                    1      1  0.901163  \n",
       "996                    0      1 -0.483111  \n",
       "999                    1      1  0.490147  \n",
       "1061                   0      1 -0.712805  \n",
       "1063                   1      0  0.423855  \n",
       "1070                   0      0 -0.464231  \n",
       "1074                   1      0  0.496532  \n",
       "1116                   0      1 -0.402821  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_df[abs(test_df['gap'])> 0.0][['matchid', 'gs', 'gd', 'fix_result', 'rq_r', 'rq', 'new_offset_val', 'offset_result', 'pred_offset_result', 'check', 'gap']]\n",
    "b[['matchid', 'gs', 'gd', 'fix_result', 'rq', 'new_offset_val', 'offset_result', 'pred_offset_result', 'check', 'gap']]\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30980\n",
      "[0]\ttrain-merror:0.46384\tval-merror:0.536404\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 100 rounds.\n",
      "[1]\ttrain-merror:0.45606\tval-merror:0.536404\n",
      "[2]\ttrain-merror:0.453146\tval-merror:0.534918\n",
      "[3]\ttrain-merror:0.453695\tval-merror:0.533432\n",
      "[4]\ttrain-merror:0.453935\tval-merror:0.523031\n",
      "[5]\ttrain-merror:0.455066\tval-merror:0.526003\n",
      "[6]\ttrain-merror:0.454209\tval-merror:0.534918\n",
      "[7]\ttrain-merror:0.452667\tval-merror:0.531946\n",
      "[8]\ttrain-merror:0.450747\tval-merror:0.523031\n",
      "[9]\ttrain-merror:0.45061\tval-merror:0.524517\n",
      "[10]\ttrain-merror:0.450644\tval-merror:0.524517\n",
      "[11]\ttrain-merror:0.450781\tval-merror:0.527489\n",
      "[12]\ttrain-merror:0.450713\tval-merror:0.530461\n",
      "[13]\ttrain-merror:0.451056\tval-merror:0.530461\n",
      "[14]\ttrain-merror:0.450233\tval-merror:0.527489\n",
      "[15]\ttrain-merror:0.449925\tval-merror:0.524517\n",
      "[16]\ttrain-merror:0.45037\tval-merror:0.526003\n",
      "[17]\ttrain-merror:0.449685\tval-merror:0.527489\n",
      "[18]\ttrain-merror:0.450473\tval-merror:0.528975\n",
      "[19]\ttrain-merror:0.450404\tval-merror:0.527489\n",
      "[20]\ttrain-merror:0.450644\tval-merror:0.524517\n",
      "[21]\ttrain-merror:0.450233\tval-merror:0.528975\n",
      "[22]\ttrain-merror:0.44989\tval-merror:0.527489\n",
      "[23]\ttrain-merror:0.449445\tval-merror:0.524517\n",
      "[24]\ttrain-merror:0.448725\tval-merror:0.520059\n",
      "[25]\ttrain-merror:0.448896\tval-merror:0.518574\n",
      "[26]\ttrain-merror:0.449513\tval-merror:0.524517\n",
      "[27]\ttrain-merror:0.44989\tval-merror:0.521545\n",
      "[28]\ttrain-merror:0.450199\tval-merror:0.524517\n",
      "[29]\ttrain-merror:0.450576\tval-merror:0.520059\n",
      "[30]\ttrain-merror:0.450884\tval-merror:0.523031\n",
      "[31]\ttrain-merror:0.450919\tval-merror:0.521545\n",
      "[32]\ttrain-merror:0.45109\tval-merror:0.521545\n",
      "[33]\ttrain-merror:0.450679\tval-merror:0.523031\n",
      "[34]\ttrain-merror:0.450439\tval-merror:0.526003\n",
      "[35]\ttrain-merror:0.450884\tval-merror:0.527489\n",
      "[36]\ttrain-merror:0.450953\tval-merror:0.523031\n",
      "[37]\ttrain-merror:0.451501\tval-merror:0.523031\n",
      "[38]\ttrain-merror:0.450713\tval-merror:0.527489\n",
      "[39]\ttrain-merror:0.450302\tval-merror:0.526003\n",
      "[40]\ttrain-merror:0.450096\tval-merror:0.521545\n",
      "[41]\ttrain-merror:0.450062\tval-merror:0.523031\n",
      "[42]\ttrain-merror:0.449445\tval-merror:0.523031\n",
      "[43]\ttrain-merror:0.44965\tval-merror:0.523031\n",
      "[44]\ttrain-merror:0.450027\tval-merror:0.526003\n",
      "[45]\ttrain-merror:0.450096\tval-merror:0.520059\n",
      "[46]\ttrain-merror:0.449993\tval-merror:0.526003\n",
      "[47]\ttrain-merror:0.449959\tval-merror:0.527489\n",
      "[48]\ttrain-merror:0.449959\tval-merror:0.518574\n",
      "[49]\ttrain-merror:0.450439\tval-merror:0.523031\n",
      "[50]\ttrain-merror:0.450336\tval-merror:0.518574\n",
      "[51]\ttrain-merror:0.450302\tval-merror:0.520059\n",
      "[52]\ttrain-merror:0.450404\tval-merror:0.520059\n",
      "[53]\ttrain-merror:0.450267\tval-merror:0.520059\n",
      "[54]\ttrain-merror:0.449993\tval-merror:0.521545\n",
      "[55]\ttrain-merror:0.450062\tval-merror:0.524517\n",
      "[56]\ttrain-merror:0.449719\tval-merror:0.520059\n",
      "[57]\ttrain-merror:0.449273\tval-merror:0.518574\n",
      "[58]\ttrain-merror:0.448965\tval-merror:0.517088\n",
      "[59]\ttrain-merror:0.448862\tval-merror:0.514116\n",
      "[60]\ttrain-merror:0.448999\tval-merror:0.514116\n",
      "[61]\ttrain-merror:0.448588\tval-merror:0.515602\n",
      "[62]\ttrain-merror:0.448554\tval-merror:0.514116\n",
      "[63]\ttrain-merror:0.448279\tval-merror:0.514116\n",
      "[64]\ttrain-merror:0.448348\tval-merror:0.515602\n",
      "[65]\ttrain-merror:0.448177\tval-merror:0.51263\n",
      "[66]\ttrain-merror:0.448245\tval-merror:0.511144\n",
      "[67]\ttrain-merror:0.448108\tval-merror:0.509658\n",
      "[68]\ttrain-merror:0.448142\tval-merror:0.511144\n",
      "[69]\ttrain-merror:0.448382\tval-merror:0.511144\n",
      "[70]\ttrain-merror:0.447902\tval-merror:0.511144\n",
      "[71]\ttrain-merror:0.447765\tval-merror:0.51263\n",
      "[72]\ttrain-merror:0.447697\tval-merror:0.511144\n",
      "[73]\ttrain-merror:0.447491\tval-merror:0.511144\n",
      "[74]\ttrain-merror:0.447423\tval-merror:0.511144\n",
      "[75]\ttrain-merror:0.447011\tval-merror:0.514116\n",
      "[76]\ttrain-merror:0.446566\tval-merror:0.514116\n",
      "[77]\ttrain-merror:0.446531\tval-merror:0.517088\n",
      "[78]\ttrain-merror:0.446463\tval-merror:0.515602\n",
      "[79]\ttrain-merror:0.446908\tval-merror:0.514116\n",
      "[80]\ttrain-merror:0.446497\tval-merror:0.514116\n",
      "[81]\ttrain-merror:0.446634\tval-merror:0.514116\n",
      "[82]\ttrain-merror:0.44636\tval-merror:0.514116\n",
      "[83]\ttrain-merror:0.445983\tval-merror:0.51263\n",
      "[84]\ttrain-merror:0.445983\tval-merror:0.511144\n",
      "[85]\ttrain-merror:0.445606\tval-merror:0.511144\n",
      "[86]\ttrain-merror:0.445503\tval-merror:0.517088\n",
      "[87]\ttrain-merror:0.445263\tval-merror:0.515602\n",
      "[88]\ttrain-merror:0.445332\tval-merror:0.517088\n",
      "[89]\ttrain-merror:0.44516\tval-merror:0.517088\n",
      "[90]\ttrain-merror:0.445023\tval-merror:0.517088\n",
      "[91]\ttrain-merror:0.445058\tval-merror:0.517088\n",
      "[92]\ttrain-merror:0.445126\tval-merror:0.515602\n",
      "[93]\ttrain-merror:0.44516\tval-merror:0.515602\n",
      "[94]\ttrain-merror:0.445332\tval-merror:0.517088\n",
      "[95]\ttrain-merror:0.445366\tval-merror:0.520059\n",
      "[96]\ttrain-merror:0.444955\tval-merror:0.521545\n",
      "[97]\ttrain-merror:0.444475\tval-merror:0.515602\n",
      "[98]\ttrain-merror:0.444372\tval-merror:0.517088\n",
      "[99]\ttrain-merror:0.444132\tval-merror:0.518574\n",
      "[100]\ttrain-merror:0.444304\tval-merror:0.518574\n",
      "[101]\ttrain-merror:0.444064\tval-merror:0.518574\n",
      "[102]\ttrain-merror:0.443961\tval-merror:0.517088\n",
      "[103]\ttrain-merror:0.443687\tval-merror:0.514116\n",
      "[104]\ttrain-merror:0.443378\tval-merror:0.511144\n",
      "[105]\ttrain-merror:0.443447\tval-merror:0.51263\n",
      "[106]\ttrain-merror:0.443481\tval-merror:0.509658\n",
      "[107]\ttrain-merror:0.443721\tval-merror:0.51263\n",
      "[108]\ttrain-merror:0.443755\tval-merror:0.514116\n",
      "[109]\ttrain-merror:0.443858\tval-merror:0.51263\n",
      "[110]\ttrain-merror:0.44331\tval-merror:0.51263\n",
      "[111]\ttrain-merror:0.443378\tval-merror:0.51263\n",
      "[112]\ttrain-merror:0.443481\tval-merror:0.515602\n",
      "[113]\ttrain-merror:0.443549\tval-merror:0.517088\n",
      "[114]\ttrain-merror:0.443378\tval-merror:0.515602\n",
      "[115]\ttrain-merror:0.443447\tval-merror:0.515602\n",
      "[116]\ttrain-merror:0.443104\tval-merror:0.515602\n",
      "[117]\ttrain-merror:0.442864\tval-merror:0.515602\n",
      "[118]\ttrain-merror:0.442898\tval-merror:0.517088\n",
      "[119]\ttrain-merror:0.443001\tval-merror:0.51263\n",
      "[120]\ttrain-merror:0.442933\tval-merror:0.514116\n",
      "[121]\ttrain-merror:0.442693\tval-merror:0.517088\n",
      "[122]\ttrain-merror:0.442418\tval-merror:0.520059\n",
      "[123]\ttrain-merror:0.442179\tval-merror:0.517088\n",
      "[124]\ttrain-merror:0.442418\tval-merror:0.517088\n",
      "[125]\ttrain-merror:0.442247\tval-merror:0.518574\n",
      "[126]\ttrain-merror:0.442076\tval-merror:0.520059\n",
      "[127]\ttrain-merror:0.442213\tval-merror:0.518574\n",
      "[128]\ttrain-merror:0.442281\tval-merror:0.518574\n",
      "[129]\ttrain-merror:0.442179\tval-merror:0.515602\n",
      "[130]\ttrain-merror:0.442076\tval-merror:0.517088\n",
      "[131]\ttrain-merror:0.441939\tval-merror:0.515602\n",
      "[132]\ttrain-merror:0.442007\tval-merror:0.515602\n",
      "[133]\ttrain-merror:0.442041\tval-merror:0.515602\n",
      "[134]\ttrain-merror:0.441973\tval-merror:0.515602\n",
      "[135]\ttrain-merror:0.441973\tval-merror:0.515602\n",
      "[136]\ttrain-merror:0.441801\tval-merror:0.515602\n",
      "[137]\ttrain-merror:0.441904\tval-merror:0.514116\n",
      "[138]\ttrain-merror:0.441287\tval-merror:0.514116\n",
      "[139]\ttrain-merror:0.441013\tval-merror:0.517088\n",
      "[140]\ttrain-merror:0.441013\tval-merror:0.517088\n",
      "[141]\ttrain-merror:0.441047\tval-merror:0.517088\n",
      "[142]\ttrain-merror:0.440808\tval-merror:0.515602\n",
      "[143]\ttrain-merror:0.440636\tval-merror:0.515602\n",
      "[144]\ttrain-merror:0.440568\tval-merror:0.515602\n",
      "[145]\ttrain-merror:0.440465\tval-merror:0.515602\n",
      "[146]\ttrain-merror:0.440773\tval-merror:0.515602\n",
      "[147]\ttrain-merror:0.440602\tval-merror:0.515602\n",
      "[148]\ttrain-merror:0.440191\tval-merror:0.517088\n",
      "[149]\ttrain-merror:0.440259\tval-merror:0.517088\n",
      "[150]\ttrain-merror:0.439985\tval-merror:0.518574\n",
      "[151]\ttrain-merror:0.439848\tval-merror:0.518574\n",
      "[152]\ttrain-merror:0.439471\tval-merror:0.518574\n",
      "[153]\ttrain-merror:0.439231\tval-merror:0.518574\n",
      "[154]\ttrain-merror:0.439437\tval-merror:0.518574\n",
      "[155]\ttrain-merror:0.439505\tval-merror:0.518574\n",
      "[156]\ttrain-merror:0.439402\tval-merror:0.518574\n",
      "[157]\ttrain-merror:0.439368\tval-merror:0.517088\n",
      "[158]\ttrain-merror:0.439059\tval-merror:0.518574\n",
      "[159]\ttrain-merror:0.439059\tval-merror:0.517088\n",
      "[160]\ttrain-merror:0.438648\tval-merror:0.518574\n",
      "[161]\ttrain-merror:0.438648\tval-merror:0.520059\n",
      "[162]\ttrain-merror:0.438511\tval-merror:0.518574\n",
      "[163]\ttrain-merror:0.438511\tval-merror:0.517088\n",
      "[164]\ttrain-merror:0.438477\tval-merror:0.518574\n",
      "[165]\ttrain-merror:0.438614\tval-merror:0.515602\n",
      "[166]\ttrain-merror:0.438511\tval-merror:0.515602\n",
      "[167]\ttrain-merror:0.438682\tval-merror:0.517088\n",
      "Stopping. Best iteration:\n",
      "[67]\ttrain-merror:0.448108\tval-merror:0.509658\n",
      "\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "x_columns = [\n",
    "#     'win_bet_return', 'draw_bet_return', 'lose_bet_return', \n",
    "#     'year', 'month',\n",
    "#     'home_team_encoder', 'visit_team_encoder', 'game_encoder',\n",
    "    \n",
    "#     'h_avg_abs_gs',\n",
    "#     'h_avg_abs_gd', 'v_avg_abs_gs', 'v_avg_abs_gd', 'h_avg_abs_win',\n",
    "#     'h_avg_abs_draw', 'h_avg_abs_lose', 'v_avg_abs_win',\n",
    "#     'v_avg_abs_draw', 'v_avg_abs_lose',\n",
    "    \n",
    "#     'h_0_1_gd', \n",
    "#     'h_0_1_gs', \n",
    "#     'h_0_gd', \n",
    "#     'h_0_gs', \n",
    "#     'h_1_gd', \n",
    "#     'h_1_gs',\n",
    "#     'h_2_3_gd', \n",
    "#     'h_2_3_gs', \n",
    "#     'h_2_gd', 'h_2_gs', 'h_3_gd', 'h_3_gs',\n",
    "#     'h_4_gd', 'h_4_gs', 'h_5_gd', 'h_5_gs', 'h_6_gd', 'h_6_gs',\n",
    "#     'h_7_gd', 'h_7_gs', 'h_ab_4_gd', 'h_ab_4_gs', 'h_abs_avg_g',\n",
    "#     'h_abs_avg_gd', 'h_abs_avg_gs', 'h_abs_draw', 'h_abs_g',\n",
    "#     'h_abs_gd', 'h_abs_gs', 'h_abs_lose', 'h_abs_win', 'h_count',\n",
    "#     'h_host_count', 'h_host_draw', 'h_host_g', 'h_host_gd',\n",
    "#     'h_host_gs', 'h_host_lose', 'h_host_win', \n",
    "    \n",
    "#     'v_0_1_gd',\n",
    "#     'v_0_1_gs', 'v_0_gd', 'v_0_gs', 'v_1_gd', 'v_1_gs', 'v_2_3_gd',\n",
    "#     'v_2_3_gs', 'v_2_gd', 'v_2_gs', 'v_3_gd', 'v_3_gs', 'v_4_gd',\n",
    "#     'v_4_gs', 'v_5_gd', 'v_5_gs', 'v_6_gd', 'v_6_gs', 'v_7_gd',\n",
    "#     'v_7_gs', 'v_ab_4_gd', 'v_ab_4_gs', 'v_abs_avg_g', 'v_abs_avg_gd',\n",
    "#     'v_abs_avg_gs', 'v_abs_draw', 'v_abs_g', 'v_abs_gd', 'v_abs_gs',\n",
    "#     'v_abs_lose', 'v_abs_win', \n",
    "    \n",
    "#     'v_count', \n",
    "#     'v_visit_count',\n",
    "#     'v_visit_draw', 'v_visit_g', 'v_visit_gd', 'v_visit_gs',\n",
    "#     'v_visit_lose', 'v_visit_win',\n",
    "    \n",
    "#     'h_0_1_gd_rate', 'h_0_1_gs_rate',\n",
    "#     'h_0_gd_rate', 'h_0_gs_rate', 'h_1_gd_rate', 'h_1_gs_rate',\n",
    "#     'h_2_3_gd_rate', 'h_2_3_gs_rate', 'h_2_gd_rate', 'h_2_gs_rate',\n",
    "#     'h_3_gd_rate', 'h_3_gs_rate', 'h_4_gd_rate', 'h_4_gs_rate',\n",
    "#     'h_5_gd_rate', 'h_5_gs_rate', 'h_6_gd_rate', 'h_6_gs_rate',\n",
    "#     'h_7_gd_rate', 'h_7_gs_rate', 'h_ab_4_gd_rate', 'h_ab_4_gs_rate',\n",
    "#     'h_abs_draw_rate', 'h_abs_lose_rate', 'h_abs_win_rate',\n",
    "#     'v_0_1_gd_rate', 'v_0_1_gs_rate', 'v_0_gd_rate', 'v_0_gs_rate',\n",
    "#     'v_1_gd_rate', 'v_1_gs_rate', 'v_2_3_gd_rate', 'v_2_3_gs_rate',\n",
    "#     'v_2_gd_rate', 'v_2_gs_rate', 'v_3_gd_rate', 'v_3_gs_rate',\n",
    "#     'v_4_gd_rate', 'v_4_gs_rate', 'v_5_gd_rate', 'v_5_gs_rate',\n",
    "#     'v_6_gd_rate', 'v_6_gs_rate', 'v_7_gd_rate', 'v_7_gs_rate',\n",
    "    \n",
    "#     'v_ab_4_gd_rate', \n",
    "#     'v_ab_4_gs_rate', \n",
    "#     'v_abs_draw_rate',\n",
    "#     'v_abs_lose_rate', \n",
    "#     'v_abs_win_rate',\n",
    "#     'h_host_draw_rate',\n",
    "#     'h_host_g_rate', \n",
    "#     'h_host_gd_rate', \n",
    "#     'h_host_gs_rate',\n",
    "#     'h_host_lose_rate', \n",
    "#     'h_host_win_rate', \n",
    "#     'v_visit_draw_rate',\n",
    "#     'v_visit_g_rate', \n",
    "#     'v_visit_gd_rate', \n",
    "#     'v_visit_gs_rate',\n",
    "#     'v_visit_lose_rate', \n",
    "#     'v_visit_win_rate',\n",
    "    \n",
    "    'avg_init_draw_odd',\n",
    "    'avg_init_lose_odd', 'avg_init_win_odd', 'avg_new_draw_kelly',\n",
    "    'avg_new_draw_odd', 'avg_new_draw_rate', 'avg_new_lose_kelly',\n",
    "    'avg_new_lose_odd', 'avg_new_lose_rate', 'avg_new_win_kelly',\n",
    "    'avg_new_win_odd', 'avg_new_win_rate', 'avg_pay_rate',\n",
    "    'dispersion_draw', 'dispersion_lose', 'dispersion_win', 'id_y',\n",
    "    'max_init_draw_odd', 'max_init_lose_odd', 'max_init_win_odd',\n",
    "    'max_new_draw_kelly', 'max_new_draw_odd', 'max_new_draw_rate',\n",
    "    'max_new_lose_kelly', 'max_new_lose_odd', 'max_new_lose_rate',\n",
    "    'max_new_win_kelly', 'max_new_win_odd', 'max_new_win_rate',\n",
    "    'max_pay_rate', 'min_init_draw_odd', 'min_init_lose_odd',\n",
    "    'min_init_win_odd', 'min_new_draw_kelly', 'min_new_draw_odd',\n",
    "    'min_new_draw_rate', 'min_new_lose_kelly', 'min_new_lose_odd',\n",
    "    'min_new_lose_rate', 'min_new_win_kelly', 'min_new_win_odd',\n",
    "    'min_new_win_rate', 'min_pay_rate', 'std_draw', 'std_lose',\n",
    "    'std_win'\n",
    "]\n",
    "    \n",
    "params={\n",
    "    'booster':'gbtree',\n",
    "    # 这里手写数字是0-9，是一个多类的问题，因此采用了multisoft多分类器，\n",
    "    'objective': 'multi:softmax', \n",
    "#     'objective': 'multi:softprob',\n",
    "    'num_class':3, # 类数，与 multisoftmax 并用\n",
    "    \n",
    "    'gamma':0.01,  # 在树的叶子节点下一个分区的最小损失，越大算法模型越保守 。[0:]\n",
    "    \n",
    "    'max_depth':8, # 构建树的深度 [1:]\n",
    "    \n",
    "    #'lambda':450,  # L2 正则项权重\n",
    "    'subsample':0.7, # 采样训练数据，设置为0.5，随机选择一般的数据实例 (0:1]\n",
    "    'colsample_bytree':0.7, # 构建树树时的采样比率 (0:1]\n",
    "    #'min_child_weight':12, # 节点的最少特征数\n",
    "    'silent':1 ,\n",
    "    \n",
    "#     这部分需要调整\n",
    "#     'eta': 0.05, # 如同学习率\n",
    "    'eta': 0.01, # 如同学习率\n",
    "    \n",
    "    \n",
    "    'seed':2018,\n",
    "    'nthread':4,# cpu 线程数,根据自己U的个数适当调整\n",
    "}\n",
    "\n",
    "# t = train_dataset_df[\n",
    "#     (train_dataset_df['win_bet_return'] <= 2) |\n",
    "#     (train_dataset_df['draw_bet_return'] <= 2) |\n",
    "#     (train_dataset_df['lose_bet_return'] <= 2)\n",
    "# ]\n",
    "\n",
    "t = train_dataset_df\n",
    "\n",
    "print(len(t))\n",
    "\n",
    "train_dataset = t[t['year'] < 2019]\n",
    "test_dataset = t[t['year'] == 2019]\n",
    "\n",
    "valid_dataset = test_dataset[test_dataset['month'] < 3]\n",
    "test_dataset = test_dataset[test_dataset['month'] >= 3]\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_dataset[x_columns], label=train_dataset['fix_result'])\n",
    "xgtest = xgb.DMatrix(test_dataset[x_columns], label=test_dataset['fix_result'])\n",
    "xgvalid = xgb.DMatrix(valid_dataset[x_columns], label=valid_dataset['fix_result'])\n",
    "\n",
    "watchlist = [(xgtrain, 'train'),(xgvalid, 'val')]\n",
    "\n",
    "num_rounds = 10000\n",
    "stop_rounds = 100\n",
    "\n",
    "# num_rounds = 10000\n",
    "# stop_rounds = 300\n",
    "\n",
    "\n",
    "model = xgb.train(params, xgtrain, num_rounds, watchlist,early_stopping_rounds=stop_rounds)\n",
    "print(model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3452244 , 0.3264327 , 0.3283429 ],\n",
       "       [0.3335006 , 0.3313826 , 0.33511677],\n",
       "       [0.32226524, 0.32644787, 0.35128686],\n",
       "       ...,\n",
       "       [0.32169273, 0.32734713, 0.3509601 ],\n",
       "       [0.32342005, 0.32687503, 0.34970492],\n",
       "       [0.34933007, 0.32826293, 0.32240704]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求可能性的时候用：'objective': 'multi:softprob',\n",
    "\n",
    "pred_probs = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['fix_result'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(items):\n",
    "    if items[0] <= items[1] and items[0] <= items[2]:\n",
    "        return [1,2]\n",
    "    elif items[1] <= items[0] and items[1] <= items[2]:\n",
    "        return [0,2]\n",
    "    elif items[2] <= items[0] and items[2] <= items[1]:\n",
    "        return [0,1]\n",
    "    \n",
    "fix_results = test_dataset['fix_result'].values\n",
    "\n",
    "results = []\n",
    "for i in range(len(pred_probs)):\n",
    "    items = pred_probs[i]\n",
    "    probs = get_result(items)\n",
    "    \n",
    "    if fix_results[i] in probs:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7400530503978779"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(results).sum() / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4880636604774536"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "preds\n",
    "\n",
    "accuracy_score(test_dataset['fix_result'], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM多分类训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leewind/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/leewind/.local/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/home/leewind/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "# Seperating Predictors and Outcome values from train and test sets\n",
    "X_train = train_dataset[x_columns]\n",
    "Y_train_label = train_dataset['fix_result'].values.astype(object)\n",
    "\n",
    "X_test = test_dataset[x_columns]\n",
    "Y_test_label = test_dataset['fix_result'].values.astype(object)\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# encoding train labels \n",
    "encoder.fit(Y_train_label)\n",
    "Y_train = encoder.transform(Y_train_label)\n",
    "\n",
    "# encoding test labels \n",
    "encoder.fit(Y_test_label)\n",
    "Y_test = encoder.transform(Y_test_label)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "#                      'C': [1, 10, 100, 1000]},\n",
    "#                     {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "params_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [10, 100]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# svm_model = GridSearchCV(SVC(), params_grid, cv=5)\n",
    "# svm_model.fit(X_train_scaled, Y_train)\n",
    "final_model = SVC(C=1, kernel='rbf', degree=3, gamma='auto', verbose=True)\n",
    "final_model.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score for SVM: 0.509186\n",
      "Testing  set score for SVM: 0.503095\n"
     ]
    }
   ],
   "source": [
    "# final_model = svm_model.best_estimator_\n",
    "\n",
    "print(\"Training set score for SVM: %f\" % final_model.score(X_train_scaled , Y_train))\n",
    "print(\"Testing  set score for SVM: %f\" % final_model.score(X_test_scaled  , Y_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
